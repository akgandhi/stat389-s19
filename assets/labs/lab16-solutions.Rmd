---
title: "Lab 16: Training Neural Networks"
author: ""
output: html_document
---

```{r, message=FALSE}
library(readr)
library(ggplot2)
library(dplyr)
library(glmnet)

theme_set(theme_minimal())
```

This lab has three parts. In the first, you will do a very simple SGD algorithm
to better understand how it performs (just run the code; nothing to change but I
want you to make sure you understand it). Then, you'll apply your derivation on
the exercises to train a neural network using your own SGD code. Finally, use
keras to train a neural network.

## SGD

Let's generate data for simple linear regression task (we will only estimate a
slope to simply things further):

```{r}
set.seed(1)

x <- runif(25)
y <- x * 2 + rnorm(25, sd=0.3)
```

If we use mean squared error as the loss, the derivative is given by:

$$ \frac{d L}{d b} = \frac{d}{d b} \sum_i (y_i - b \cdot x_i)^2 = \sum_{i} 2 (y_i - b x_i) \cdot (-1) = \sum_{i} 2 \cdot (b \cdot x_i - y_i)  $$
The contribution of the $i$'th data point to the derivative is:

$$ 2 (b \cdot x_i - y). $$

To find the optimal value with gradient descent we could run the following:

```{r}
num_iter <- 5
tol <- 1e-6
rho <- 0.03

b <- 0
for (iter in seq_len(num_iter))
{
  b <- b - rho * 2 * sum(b * x - y)
}

b
```

Now, let's modify this to run SGD:

```{r}
num_iter <- 5
tol <- 1e-6
rho <- 0.03

b <- 0
for (iter in seq_len(num_iter))
{
  for (snum in seq_along(y))
  {
    b <- b - rho * 2 * (b * x[snum] - y[snum])
  }
}

b
```

## SGD and Neural network

Now, I'll generate new dataset with input variables:

```{r}
set.seed(1)

X <- matrix(runif(100 * 3), ncol = 3)
y <- X[,1] + sin(X[,2] * 2) + rnorm(100, sd=0.01)
```

Now, let's define our activation function and its derivative:

```{r}
sigma <- function(v) { as.numeric(v * (v > 0)) }
sigma_prime <- function(v) { sign(v) *  (v > 0) }
```

Here's a sketch of the SGD code to run a neural network. Fill in the missing values
and run it. The plot will hopefully verify that everything is working well.

```{r}
# parameters for the algorithm
num_iter <- 25
tol <- 1e-6
rho <- 0.03
p <- 10             # number of values in the hidden layer, z

# hidden layer parameters
alpha <- runif(p)
B <- matrix(runif(p * ncol(X)), nrow=ncol(X))

# output layer parameters
c <- 0
gamma <- runif(p)

# you need to fill in in the four missing values
for (iter in seq_len(num_iter))
{
  for (snum in seq_along(y))
  {
    # update w, a, and z
    z <- alpha + t(B) %*% X[snum, ]
    a <- sigma(z)
    w <- as.numeric(c + gamma %*% a)

    c <- c - rho * (w - y[snum])
    for (k in seq(p))
    {
      alpha[k] <- alpha[k] - rho * (w - y[snum]) * gamma[k] * sigma_prime(z[k])
      for (j in seq_len(ncol(X)))
      {
        B[j, k] <- B[j, k] - rho * (w - y[snum]) * gamma[k] * sigma_prime(z[k]) * X[snum, j]
      }
      gamma[k] <- gamma[k] - rho * (w - y[snum]) * a[k]
    }
  }

}

# predict values for y
y_pred <- rep(0, length(y))
for (snum in seq_along(y))
{
  z <- alpha + t(B) %*% X[snum, ]
  a <- sigma(z)
  w <- as.numeric(c + gamma %*% a)
  y_pred[snum] <- w
}

# plot the results to see that they are reasonable:
ggplot(tibble(y = y, yhat = y_pred), aes(y, yhat)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0)
```

## Data prediction task

Today, I want you to estimate the quality of various wines based on properties of the
wines. Make use of at least one neural network, but ultimately you can submit whatever
model you think is best.

```{r}
wine <- read_csv("https://statsmaths.github.io/ml_data/wine.csv")
ocol <- ncol(wine)
```

Create the training data:

```{r}
X <- scale(model.matrix( ~ -1 + ., data = wine[,seq(4, ocol)]))
y <- wine$quality

X_train <- X[wine$train_id == "train",]
X_valid <- X[wine$train_id == "valid",]
y_train <- y[wine$train_id == "train"]
y_valid <- y[wine$train_id == "valid"]
```

Create the model, compile, and train

```{r}
library(keras)

model <- keras_model_sequential()
model %>%
  layer_dense(units = c(64), input_shape = ncol(X)) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = c(64), input_shape = ncol(X)) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 1)

model %>% compile(
  loss = 'mse',
  optimizer = optimizer_rmsprop(),
  metrics = c('mse')
)

model %>%
  fit(X_train, y_train, epochs = 5,
      validation_data = list(X_valid, y_valid))
```

Now, fit the data and evaluate:

```{r}
pred <- predict(model, X)
sqrt(tapply((wine$quality - pred)^2, wine$train_id, mean))
```


## Submitting your solutions

Finally, once you have your predictions saved as a variable called `pred`,
run the following code to produce your your results:

```{r}
submit <- select(wine, obs_id, train_id)
submit <- mutate(submit, pred = pred)
write_csv(submit, "class16_submit.csv")
```

Now, upload the csv file to GitHub and you are done.
