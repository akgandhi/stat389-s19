---
title: "Lab 14"
author: ""
output: html_document
---

# Set-up

Read in the following libraries and to load the metadata about
the flowers:

```{r}
library(readr)
library(dplyr)
library(glmnet)

f17 <- read_csv("https://statsmaths.github.io/ml_data/flowers_17.csv")
```

To get the actual data for the images, you'll have to download the following
file.

- https://github.com/statsmaths/ml_data/releases/download/v1/flowers_17_x64.rds

Once this is downloaded, you'll have to run something like this:

```{r}
x64 <- read_rds("../notes/image_data//flowers_17_x64.rds")
```

If, for example, the file sits on your Desktop and you have a Mac. If you have
trouble with any of this, please let me know as soon as possible.


# Todays lab

The only thing you need to do today is to play around with different models
and see how well they work. Once you are satisfied, pick the best model and
incorporate the predictions into your output in the final chunk below. Note:
I would suggest keeping each model in its own code chunk for readability.
Also, save your work frequently as it is quite likely that you will crash
R at some point.

I am going to try two different models. One with glmnet and the raw pixels
and another with the extracted color counts. Here is the data collapsed into
a single matrix:

```{r}
X <- t(apply(x64, 1, cbind))
y <- f17$class

X_train <- X[f17$train_id == "train",]
y_train <- y[f17$train_id == "train"]
dim(X_train)
```

Now, I'll use **glmnet** to fit a multinomial model on the dataset. There are a large
number of categories and a lot of variables, so this may take several minutes to complete.

```{r}
model <- cv.glmnet(X_train, y_train, family = "multinomial", nfolds = 3)
pred <- predict(model, newx = X, type = "class")
tapply(pred == y, f17$train_id, mean)
```

Next, let's compute the color counts:

```{r}
color_vals <- c(hsv(1, 0, seq(0, 1, by = 0.2)),
                hsv(seq(0, 0.9, by = 0.1), 1, 1))
X_hsv <- matrix(0, ncol = length(color_vals),
                   nrow = nrow(f17))
for (i in seq_len(nrow(f17))) {
  red <- as.numeric(x64[i,,,1])
  green <- as.numeric(x64[i,,,2])
  blue <- as.numeric(x64[i,,,3])
  hsv <- t(rgb2hsv(red, green, blue, maxColorValue = 1))

  color <- rep("#000000", nrow(hsv))

  index <- which(hsv[,2] < 0.2)
  color[index] <- hsv(1, 0, round(hsv[index,3] * 5) / 5)

  index <- which(hsv[,2] > 0.2 & hsv[,3] > 0.2)
  color[index] <- hsv(round(hsv[index,1],1), 1, 1)

  X_hsv[i,] <- table(factor(color, levels = color_vals))
}
```

Using this data, construct training and validation data:

```{r}
y <- f17$class

X_train <- X_hsv[f17$train_id == "train",]
X_valid <- X_hsv[f17$train_id == "valid",]
y_train <- y[f17$train_id == "train"]
y_valid <- y[f17$train_id == "valid"]
```

And fit the glmnet model:

```{r}
model <- cv.glmnet(X_train, y_train, family = "multinomial")

pred <- as.numeric(predict(model, newx = X_hsv,
                           type = "class"))

tapply(pred == y, f17$train_id, mean)
```

I am getting about 1/3 of the flowers correct, not terrible given that there
are 17 total classes (random guessing is only correct about 5% of the time).


# Submission

The code below assumes that you have added a prediction named
`pred` to every row of the dataset.

```{r}
submit <- select(f17, obs_id, train_id)
submit <- mutate(submit, pred = pred)
write_csv(submit, "class14_submit.csv")
```

Now, upload the csv file to GitHub and you are done.
