---
title: "Lab 11"
author: ""
output: html_notebook
---

```{r, message=FALSE}
library(readr)
library(ggplot2)
library(dplyr)
library(glmnet)

theme_set(theme_minimal())
```

This lab has two parts. The first has you explore the smoothing splines from
the notes. The second walks you through the first set of predictions that you
need to submit (this first prediction task is very straightforward; more of a
check that you can run through the steps than anything else).

## Install FNN

We will need one new package today. You can install it with:

```{r}
if (!require(FNN))
{
  install.packages("FNN")
}

library(FNN)
```

## Smoothing Splines

Generate the following simulated dataset to produce univariate data that has
a highly non-linear effect:

```{r, echo=FALSE}
set.seed(1)
x <- seq(1, 10, by=0.01)
y <- cos(5 * x) * 0.4 + exp(x / 10) * 2 + rnorm(length(x), sd=0.2)
y[x > 6] <- (x[x > 6] - 6) * -0.5 + 4 + rnorm(sum(x > 6), sd=0.2)

df <- tibble(x = x, y = y)
ggplot(df, aes(x, y)) +
  geom_point()
```

1. Apply linear regression with a polynomial basis expansion to this dataset. Just using
plots, what looks to be the ideal order of the polynomial to predict the dataset? Do
you run into any numerical issues?

```{r}

```

2. Now, apply a cubic regression spline using the code from my notes. Try several
different numbers of knots and see which ones appear to be doing the best job
with the prediction task. 

```{r}

```

3. Use the glmnet package to fit a regression spline with a large number of knots
under an elastic net penalty. Compare the results to those using no penalization.

```{r}

```

4. Fit a smoothing spline through the function `smooth.spline` to the dataset.
How well does it seem to do compared to the other models above?

```{r}

```

5. Finally, fit k-nearest neighbors to the dataset. Try several different values for
k and try to settle on an optimal value. Compare the values of the curve to the splines
in the first 4 questions.

```{r}

```

## Read in mammal sleep data

Today I will have you look at a dataset of California housing prices by census
tract. We will look at this set again next week; today I am going to remove some of
the variables and leave you with only one (latitude) to do predictions on:

```{r, message=FALSE}
housing <- read_csv("https://github.com/statsmaths/ml_data/blob/master/ca_house_price.csv?raw=true")
housing <- housing[, c(1:3, 5L)]
```

The format of this dataset is the same as all of the others that we will have
going forward. The first column is an id that I need in order to check your 
results. The second column splits the data into a training and testing set.
The third column is the variable that you are going to try to predict:

```{r}
housing
```

Notice that the variable `median_house_value` is actually missing in the test set.
You need to submit your solutions through GitHub and I will test how well you did.
It is much easier to have you submit solutions to the entire dataset and not just the
test set, so that is what I will have you do.

Your task is to make predictions for the variable `median_house_value`. You should try
a number of models, fitting them on the training set, and then evaluate how well them
perform on the validation set. Select the best model and save it to the variable `prediction`
and then run the code at the bottom of this script and follow the instructions.

Just to get you started, here is a plot of the data:

```{r}
filter(housing, train_id == "train") %>%
  ggplot(aes(latitude, median_house_value)) +
    geom_point()
```

Notice that the relationship is non-linear and fairly difficult to visualize
compared to our simulated dataset. You can at least, for example, see the 
peaks corresponding to Los Angeles and San Francisco.

As this is our first programming lab like this, I'll get you started by giving the
code for the three main estimators. You still need to figure out the best tuning
parameter and choose between the three models.

```{r}
# pre-process the dataset
x <- matrix(housing$latitude, ncol=1)
y <- housing$median_house_value
x_train <- x[housing$train_id == "train",]
y_train <- y[housing$train_id == "train"]
tfl <- housing$train_id == "train"
```

```{r}
# Regression splines (set P)
P <- 3
knots <- quantile(x_train, seq(0.1, 0.9, length.out = P))

B <- matrix(0, nrow = length(x), ncol = 1 + 3 + P)
B[,1] <- 1
B[,2] <- x
B[,3] <- x^2
B[,4] <- x^3

for (j in seq_len(P))
{
  B[,4 + j] <- (x - knots[j])^3 * as.numeric(x > knots[j])
}

beta <- solve(crossprod(B[tfl,], B[tfl,]), crossprod(B[tfl,], y_train), tol = 0) 
yhat <- B %*% beta

sqrt(tapply((y - yhat)^2, housing$train_id, mean)) # RMSE by group
```

```{r}
# Smoothing splines (set the parameter `spar` to control
#                    amount of smooth; number from 0 to 1)
spar <- 0.5
model <- smooth.spline(x_train, y_train, spar = spar)
yhat <- predict(model, x = x)$y

sqrt(tapply((y - yhat)^2, housing$train_id, mean)) # RMSE by group
```


```{r}
# KNN
k <- 10
yhat <- knn.reg(x_train, x, y=y_train, k=k)$pred

sqrt(tapply((y - yhat)^2, housing$train_id, mean)) # RMSE by group
```


When you have the model that you are happy with, add the predictions back into the
dataset:

```{r}
housing <- mutate(housing, pred = yhat)
```

And follow the instructions at the bottom.

## Submitting your solutions

Finally, once you have your predictions saved a variable called `awake_pred`,
run the following code to submit your 

```{r}
submit <- select(housing, obs_id, pred)
write_csv(submit, "class01_submit.csv")
```

Now, upload the csv file to GitHub and you are done.
