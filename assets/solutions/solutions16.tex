%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt,hidelinks]{article}

% 1. Load LaTeX packages
\usepackage{fontspec}
\usepackage{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xunicode}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amssymb}

% 2. Define page dimensions and spacing
\geometry{top=1in, bottom=1in, left=1in, right=1in, marginparsep=4pt,
          marginparwidth=1in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

% 3. Set header, footer, and bibliography
\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancyplain}
\fancyhf{}
\lfoot{}
\rfoot{page \thepage\ of \pageref{LastPage}}
\bibliographystyle{acm}

% 4. Set fonts for the document
\defaultfontfeatures{Mapping=tex-text}
\setromanfont{YaleNew}

% 5. Define custom code for book environments and commands
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textbf{#1}}

% 6. Define custom code for book environments and commands
\definecolor{verbgray}{gray}{0.9}
\definecolor{verbgray2}{gray}{0.975}
\definecolor{solarized@magenta}{HTML}{D33682}
\newcommand{\magenta}[1]{\textcolor{solarized@magenta}{#1}}

\lstnewenvironment{rcode}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

\lstnewenvironment{rres}{%
  \lstset{backgroundcolor=\color{verbgray2},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

% 7. Define numbering scheme for equations (only needed for handout).
\numberwithin{equation}{section}
\setcounter{section}{11}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

{\LARGE Solutions 16: Training Neural Networks}

\vspace*{18pt}

\textbf{Exercises}

Assume that we have a neural network with one hidden layer, defined as (where
$x$ is one row of the input matrix and $y$ is the corresponding output value):

\begin{align}
z_k &= \alpha_k + \sum_{j=1}^{P} B_{j, k} \cdot x_j \\
a_k &= \sigma(z_k) \\
w &= c + \sum_k \gamma_k a_k
\end{align}

Where $\sigma$ is a differentiable activation function. The terms $c$,
$\gamma_k$, and $B_{j, k}$ are the parameters that define the model.
We want to minimize the quantity (the loss function):

\begin{align}
L(w, y) &= \frac{1}{2} \cdot (w - y)^2.
\end{align}

We need to compute a number of partial derivatives, which we will do using the
chain rule. It is important that you don't jump ahead and plug things in before I
ask you to.

\textbf{Step 1}: Compute the partial derivative of:

\begin{flalign*}
\frac{\partial z_k}{\partial B_{j, k}} &= \magenta{x_j} &&
\end{flalign*}

Note that $z_k$ does not depend on $B_{j, m}$ if $m\neq k$, so we do not need
to worry about those terms.

\textbf{Step 2}: Compute the partial derivative of (yes, this is easy):

\begin{flalign*}
\frac{\partial z_k}{\partial \alpha_k} &= \magenta{1} &&
\end{flalign*}

\textbf{Step 3}: Write down a formula for the following using the notation
$\sigma'(\cdot)$ to denote the derivative of $\sigma$.

\begin{flalign*}
\frac{\partial a_k}{\partial z_k} &= \magenta{\sigma'(z_k)} &&
\end{flalign*}

\textbf{Step 4}: Write down a formula for:

\begin{flalign*}
\frac{\partial w}{\partial \gamma_k} &= \magenta{a_k} &&
\end{flalign*}

\textbf{Step 5}: What is the following (yes, this is easy also):

\begin{flalign*}
\frac{\partial w}{\partial c} &= \magenta{1} &&
\end{flalign*}

\textbf{Step 6}: Finally, what is the derivative of the loss function with
respect to $w$:

\begin{flalign*}
\frac{\partial L}{\partial w} &= \magenta{(w - y)}  &&
\end{flalign*}

\textbf{Step 7}: Notice that I can use the chain rule to write the following,
the derivative with respect to each tunable parameter in the second layer of
the model:

\begin{align}
\frac{\partial L}{\partial c} &= \frac{\partial L}{\partial w} \cdot \frac{\partial w}{\partial c} \\
\frac{\partial L}{\partial \gamma_k} &= \frac{\partial L}{\partial w} \cdot \frac{\partial w}{\partial \gamma_k}
\end{align}

Now, plug in the values that you know to compute each of these:

\begin{flalign*}
\frac{\partial L}{\partial c} &= \magenta{(w - y) \cdot 1}  = \magenta{w - y}  && \\
\frac{\partial L}{\partial \gamma_k} &= \magenta{(w - y) \cdot a_k}  &&
\end{flalign*}

\textbf{Step 8}: What about the terms $B_{j, k}$ and $\alpha_k$? They are in
the hidden layer
and require one more step:

\begin{align}
\frac{\partial L}{\partial B_{j, k}} &= \frac{\partial L}{\partial z_k} \cdot \frac{\partial z_k}{\partial B_{j, k}} \label{import} \\
&= \frac{\partial L}{\partial w} \cdot \frac{\partial w}{\partial a_k} \cdot \frac{\partial a_k}{\partial z_k} \cdot \frac{\partial z_k}{\partial B_{j, k}}
\end{align}

And:

\begin{align}
\frac{\partial L}{\partial \alpha_k} &= \frac{\partial L}{\partial z_k} \cdot \frac{\partial z_k}{\partial B_{j, k}} \label{import2} \\
&= \frac{\partial L}{\partial w} \cdot \frac{\partial w}{\partial a_k} \cdot \frac{\partial a_k}{\partial z_k} \cdot \frac{\partial z_k}{\partial \alpha_k}
\end{align}

But, you do know all of these terms. Plug them in to get the partial derivative
with respect to $B_{j, k}$ and $\alpha_k$.

\begin{flalign*}
\frac{\partial L}{\partial B_{j, k}} &= \magenta{(w - y) \cdot \gamma_k \cdot \sigma'(z_k) \cdot x_j} && \\
\frac{\partial L}{\partial \alpha_k} &= \magenta{(w - y) \cdot \gamma_k \cdot \sigma'(z_k)}
\end{flalign*}

\textbf{Summary}

The most important lines to understand here are Equations~\ref{import} and \ref{import2}.
They show the core back propagation logic: decomposing the influence of a parameter
to (i) how it influences the output of that layer and (ii) how that layer influences
the loss. This make it possible, with just a little bit more notation, to compute
gradients for the deepest of neural networks.


\end{document}

