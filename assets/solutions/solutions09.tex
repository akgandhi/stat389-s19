%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt,hidelinks]{article}

% 1. Load LaTeX packages
\usepackage{fontspec}
\usepackage{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xunicode}
\usepackage{listings}
\usepackage{color}
\usepackage{amssymb}

% 2. Define page dimensions and spacing
\geometry{top=1in, bottom=1in, left=1in, right=2in, marginparsep=4pt,
          marginparwidth=1in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

% 3. Set header, footer, and bibliography
\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancyplain}
\fancyhf{}
\lfoot{}
\rfoot{page \thepage\ of \pageref{LastPage}}
\bibliographystyle{acm}

% 4. Set fonts for the document
\defaultfontfeatures{Mapping=tex-text}
\setromanfont{YaleNew}

% 5. Define custom code for book environments and commands
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textbf{#1}}

% 6. Define custom code for book environments and commands
\definecolor{verbgray}{gray}{0.9}
\definecolor{verbgray2}{gray}{0.975}

\lstnewenvironment{rcode}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

\lstnewenvironment{rres}{%
  \lstset{backgroundcolor=\color{verbgray2},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

% 7. Define numbering scheme for equations (only needed for handout).
\numberwithin{equation}{section}
\setcounter{section}{7}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

{\LARGE Lab Solutions 09}

\vspace*{12pt}

\textbf{1. I produced the derivation of the logistic score function for you; make
sure that you can reproduce these steps yourself. Parts of this may be on the
next exam.}

\vspace*{12pt}

\textbf{2. Following the instructions in the RMarkdown file to implement logistic
regression using gradient descent.}

\vspace*{12pt}

See markdown solutions.

\vspace*{12pt}

\textbf{3. Find a formula for the second derivatives of the loss function:}
\begin{align}
\frac{\partial^2 l(b)}{\partial b_k \partial b_j}
\end{align}

\vspace*{12pt}

Using the chain run, this is mostly a straightforward calculation if you use
the compact form of $p_i$ (see supplementary file on class website):
\begin{align}
\frac{\partial^2 l(y)}{\partial \beta_k \partial \beta_j}
&= \sum_i x_{i,j} \frac{\partial p_i}{\partial \beta_k} \\
&= \sum_i x_{i,j} -1 \cdot \left( \frac{1}{1 + e^{-x^t \beta}} \right)^2 (- x_{i, k}) \cdot e^{-x_i^t \beta} \\
&= \sum_i x_{i,j} x_{i,k} \cdot \left( \frac{e^{-x^t \beta}}{1 + e^{-x^t \beta}} \right) \cdot \left( \frac{1}{1 + e^{-x^t \beta}} \right) \\
&= \sum_i x_{i,j} x_{i,k} p_i (1 - p_i).
\end{align}
The last line is also explained better in the supplementary file.

\vspace*{12pt}

\textbf{4. The Hessian matrix is a square matrix of partial derivatives. That is,
$H_{k, j} = \frac{\partial^2 l(b)}{\partial b_k \partial b_j}$. From the result
in the previous question, understand that we can write:}
\begin{align}
H &= X^t D X
\end{align}
\textbf{Where $D$ is a diagonal matrix:}
\begin{align}
D_{i, i} &= p_i (1 - p_i).
\end{align}

\vspace*{12pt}

If you do the simplification that I did of $p_i$ and $1 - p_i$, this follows
directly.

\vspace*{12pt}

\textbf{5. The ``proper'' way to logistic regression is by replacing the learning
rate $\rho$ with the Hessian matrix evaluated at the current value of $b$.
That is:}
\begin{align}
b^{(t+1)} &= b^{(t)} - H(b^{(t)}) \cdot \nabla_b l(b^{(t)}).
\end{align}
\textbf{This corresponds to doing a quadratic approximation at $b^{(t)}$ and moving
directly to the minimizing point. Implement this iteration in the RMarkdown
file and compare the convergence rate to the gradient descent implementation.}

\vspace*{12pt}

See markdown solutions.


\end{document}

