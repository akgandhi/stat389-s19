%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt,hidelinks]{article}

% 1. Load LaTeX packages
\usepackage{fontspec}
\usepackage{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xunicode}
\usepackage{listings}
\usepackage{color}
\usepackage{amssymb}

% 2. Define page dimensions and spacing
\geometry{top=1in, bottom=1in, left=1in, right=2in, marginparsep=4pt,
          marginparwidth=1in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

% 3. Set header, footer, and bibliography
\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancyplain}
\fancyhf{}
\lfoot{}
\rfoot{page \thepage\ of \pageref{LastPage}}
\bibliographystyle{acm}

% 4. Set fonts for the document
\defaultfontfeatures{Mapping=tex-text}
\setromanfont{YaleNew}

% 5. Define custom code for book environments and commands
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textbf{#1}}

% 6. Define custom code for book environments and commands
\definecolor{verbgray}{gray}{0.9}
\definecolor{verbgray2}{gray}{0.975}

\lstnewenvironment{rcode}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

\lstnewenvironment{rres}{%
  \lstset{backgroundcolor=\color{verbgray2},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

% 7. Define numbering scheme for equations (only needed for handout).
\numberwithin{equation}{section}
\setcounter{section}{6}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

{\LARGE Lab Solutions 06}

\vspace*{12pt}

\textbf{1. Take the gradient of the ridge regression loss function and set it equal
to zero. Get an equation for $\beta_{\lambda}$ in terms of $X$ and $y$. You may
need to use the fact that $b^t b$ is equal to $b^t I_p b$ in this derivation.
(Note: Do \textbf{not} yet use the SVD of $X$ here).}

\vspace*{12pt}

The gradient of the loss function is given by:
\begin{align}
\nabla_b \left[ || y - X b ||_2^2 + \lambda \cdot || b ||_2^2 \right] &=
\nabla_b \left[ y^t y + b^t (X^t X) b - 2 y^t X b + \lambda b^t I_p b \right] \\
&= 2 \cdot (X^t X) b - 2 \cdot X^t y + 2\lambda \cdot I_p b
\end{align}
Notice that the gradient of $v^t b$ is equal to $v$ and \textbf{not} $v^t$; that
seemed to cause a lot of confusion. Now, setting this equal to zero, we have:
\begin{align}
0 &= 2 \cdot (X^t X) b - 2 \cdot X^t y + 2\lambda \cdot I_p b \\
X^t y &= (X^t X) b + \lambda \cdot I_p b \\
X^t y &= \left((X^t X) + \lambda \cdot I_p \right) b \\
\left((X^t X) + \lambda \cdot I_p \right)^{-1} X^t y &= \beta_\lambda.
\end{align}
And that is all we need to do. Notice that this is the ordinary least squares
solution when $\lambda = 0$.


\vspace*{12pt}

\textbf{2. The eigenvalue decomposition of a matrix writes a matrix as $Q^t \Lambda Q$ for
a diagonal matrix $\Lambda$ (the entries are called the matrix eigenvalues) and an
orthogonal matrix $Q$. Unlike the SVD, the eigenvalue decomposition only applies to
square matrices and even then does not always exist. Show that the eigenvalues of
$X^t X$ are equal to the squared singular values of $X$.}

\vspace*{12pt}

This comes from just plugging in the SVD:
\begin{align}
X^t X &= (UDV^t)^{t} UDV^t \\
&= V D U^t U D V^t \\
&= V D^2 V^t
\end{align}
So, we have an eigen-decomposition in terms of the right singular vectors $V$
and the squared singular values $D^2$.

\vspace*{12pt}

\textbf{3. I want to derive a formula for $\beta_{\lambda}$ in terms of the SVD of X;
this is not a long derivation but does require a trick. Take the equation that you
have already derived for $\beta_{\lambda}$; there should be an identity matrix in
the formula. consider the SVD of $X$ as $UDV^t$. Write the identify matrix in the
equation as $VV^t$, plug in the SVD for $X$, and simplify. You should be able to
factor out some of the terms and are left with something very similar to equation
we had for the ordinary leasts squares estimator.}

\vspace*{12pt}

Starting with the answer to question one and plugging in the SVD we have:
\begin{align}
\beta_\lambda &= \left((X^t X) + \lambda \cdot I_p \right)^{-1} X^t y \\
&= \left( V D^2 V^t + \lambda \cdot VV^t \right)^{-1} V D U^t y \\
&= \left( V \left[D^2 + \lambda \cdot I_p \right] V^t \right)^{-1} V D U^t y \\
&= V \left[D^2 + \lambda \cdot I_p \right]^{-1} V^t V D U^t y \\
&= V \left[D^2 + \lambda \cdot I_p \right]^{-1} D U^t y \\
&= V \cdot \text{Diag} \left( \frac{\sigma_1}{\sigma_1^2 + \lambda}, \cdots, \frac{\sigma_p}{\sigma_p^2 + \lambda} \right) \cdot U^t y
\end{align}
This is just like fitting ordinary least squares, except that the middle term (which
was $D^{-1}$ is now a different diagonal matrix).

\vspace*{12pt}

\textbf{4. Understand that the ridge regression is equivalent to fitting ordinary
least squares on a new matrix $\bar{X}$ where the singular values have been
increased by a factor of $\lambda$. Given our argument about the problems with
the smallest singular value, does it make sense that this change alleviates the
problem of identifiability?}

\vspace*{12pt}

You can see from the previous answer that we have made the adjustment:
\begin{align}
\bar{\sigma_k} &\rightarrow \frac{\sigma_k^2 + \lambda}{\sigma_k}
\end{align}

\vspace*{12pt}

\textbf{5. Let's somewhat switch gears here and consider a specific example problem.
Let $p=2$ and assume that the first column of $X$ ($X_1$) can be written as:}
\begin{align}
X_1 = \alpha + X_2, \quad \alpha \in \mathbb{R}^n
\end{align}
\textbf{Where $\alpha$ is a small noise vector. So, $X_1$ and $X_2$ are very similar to
one another. Write an equation for the value $X b$, factoring in terms of $\alpha$
and $X_2$ (there should not be any $X_1$ left in the equation). Then, assume that
we have data generated by:}
\begin{align}
y &= X_2 + \text{noise}
\end{align}
\textbf{Where the noise is not too large. Convince yourself that all of the following
values of $b$ produce a reasonable estimate for $\widehat{y} = X b$:}
\begin{align}
b &= \begin{bmatrix} 0 \\ 1 \end{bmatrix} \\
b &= \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\
b &= \begin{bmatrix} -1 \\ 2 \end{bmatrix} \\
b &= \begin{bmatrix} -100 \\ 101 \end{bmatrix}
\end{align}
\textbf{What do you think is the approximate value of $\beta_\lambda$ for ridge
regression for a small value of $\lambda$ assuming the noise vector and $\alpha$
are also both small?}

\vspace*{12pt}

The value of $X b$ will be equal to just $X_1 \cdot b_1 + X_2 \cdot b_2$ (writing
out the linear regression explicitly). Simplifying we have:
\begin{align}
X b &= X_1 \cdot b_1 + X_2 \cdot b_2 \\
&= (\alpha + X_2) \cdot b_1 + X_2 \cdot b_2 \\
&= b_1 \cdot \alpha + X_2 \cdot (b_1 + b_2) \\
&\approx X_2 \cdot (b_1 + b_2)
\end{align}
Since we want:
\begin{align}
X b &\approx y = X_2 + \text{noise}
\end{align}
And solution with $b_1 + b_2 = 1$ will do the trick. Therefore all of the examples
work. For ridge regression, all of the solutions are equally predictive (more or
less), so pick the vector that has the smallest size: $b = [0.5, 0.5]$.

\vspace*{12pt}

\textbf{6. For the previous question, can you guess a plausible value for $V_p$
(the last right singular) of the matrix $X$? Look back at handout 5, question 3,
for a hint.}

\vspace*{12pt}

We know that adding a multiple of $V_p$ should not change the predictions very
much; here the $V_p$ that does that is the vector $[1, -1]$ (it adds something
to $b_1$, subtracts it from $b_2$ and keeps the sum the same). Keeping in mind
that $|| V_p ||_2 = 1$ we see that a good guess is:
\begin{align}
V_p &= \begin{bmatrix} +2^{-1/2} \\ -2^{-1/2} \end{bmatrix}
\end{align}
You could also have the signs switched (there's no way to know from the problem
which is most likely).

\vspace*{12pt}

\textbf{7. In the following final questions, we are going to consider a dataset
where:}
\begin{align}
X^t X &= 1_p \cdot n.
\end{align}
\textbf{There is nothing you need to compute here, but make sure that you understand:
(a) why it makes sense to include the $n$ on the right-hand side, and (b) why
it makes sense that we say in this case that the columns of $X$ are uncorrelated.}

\vspace*{12pt}

Consider just the first element of $X^t X$, its the size of the first column of
X: $X_1^t X_1 = || X_1 ||_2^2$. It makes sense that this increases with the sample
size. For (b), the assumption can be read as saying that the columns of $X$ are
perpendicular to one another. This geometric property is similar to the probabilistic
idea of uncorrelated.

\vspace*{12pt}

\textbf{8. Taking $X^t X = 1_p \cdot n$, what is the value of the ordinary least
squares estimator? Can you explain exactly what a particular component $\beta_k$
is in terms of an inner product?}

\vspace*{12pt}

Plugging into the formula we have for the OLS estimator, this is just:
\begin{align}
\beta &= (X^t X)^{-1} X^t y = X^t y.
\end{align}
So each value of $\beta_k$ is equal to just $X_k^t y$, the inner product of the $k$-th
column of $X$ with $y$.

\vspace*{12pt}

\textbf{9. Again taking $X^t X = 1_p \cdot n$, write the ridge regression vector
$\beta_\lambda$ as a function of just $n$, $\lambda$, and the OLS solution $\beta$.}

\vspace*{12pt}

Here, we now have:
\begin{align}
\beta_\lambda &= (X^t X + \lambda I_p)^{-1} X^t y\\
&= ((1 + \lambda) \cdot I_p)^{-1} X^t y \\
&= \frac{1}{1+\lambda} \cdot X^t y \\
&= \frac{1}{1+\lambda} \cdot \beta \\
\end{align}
So when the columns of $X$ are uncorrelated, the ridge regression just scales
the OLS solution by a factor of $(1 + \lambda)^{1}$




\end{document}

