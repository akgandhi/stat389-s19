%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt,hidelinks]{article}

% 1. Load LaTeX packages
\usepackage{fontspec}
\usepackage{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xunicode}
\usepackage{listings}
\usepackage{color}
\usepackage{amssymb}

% 2. Define page dimensions and spacing
\geometry{top=1in, bottom=1in, left=1in, right=2in, marginparsep=4pt,
          marginparwidth=1in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

% 3. Set header, footer, and bibliography
\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancyplain}
\fancyhf{}
\lfoot{}
\rfoot{page \thepage\ of \pageref{LastPage}}
\bibliographystyle{acm}

% 4. Set fonts for the document
\defaultfontfeatures{Mapping=tex-text}
\setromanfont{YaleNew}

% 5. Define custom code for book environments and commands
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textbf{#1}}

% 6. Define custom code for book environments and commands
\definecolor{verbgray}{gray}{0.9}
\definecolor{verbgray2}{gray}{0.975}

\lstnewenvironment{rcode}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

\lstnewenvironment{rres}{%
  \lstset{backgroundcolor=\color{verbgray2},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

% 7. Define numbering scheme for equations (only needed for handout).
\numberwithin{equation}{section}
\setcounter{section}{4}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

{\LARGE Lab Solutions 04}

\vspace*{12pt}

\textbf{1. Write the Equation as an inner product. Expand and distribute
the terms so that you have a loss function written a liner combination of
matrix products.}

\vspace*{12pt}

The loss function can be expanded as:
\begin{align}
\mathcal{L} &= || y - X \beta ||_2^2 \\
&= (y - X \beta)^t (y - X \beta) \\
&= (y^t - (X \beta)^t) (y - X \beta) \\
&= (y^t - \beta^t X^t) (y - X \beta) \\
&= y^t y + \beta^t X^t X \beta - \beta^t X^t y - y^t X \beta
\end{align}
Notice that all of these terms are scalar quantities, so we can replace the
value $\beta^t X^t y$ with its transpose $y^t X \beta$. This simplifies the
formula to:
\begin{align}
\mathcal{L} &= y^t y + \beta^t X^t X \beta - 2 y^t X \beta.
\end{align}

\vspace*{12pt}

\textbf{2. Convince yourself that the matrix $X^t X$ is equal to its own inverse.}

\vspace*{12pt}

This is true just because of the rules for distributing the transpose operator:
\begin{align}
(X^t X)^t &= X^t (X^t)^t = X^t X.
\end{align}

\vspace*{12pt}

\textbf{3. Use the gradient rules we had last time to compute the gradient of the
loss function for linear regression.}

\vspace*{12pt}

The gradient is given, using the results from last time, as:
\begin{align}
\nabla_\beta \mathcal{L} &= 2 X^t X \beta - 2 X^t y.
\end{align}
We already did the hard part last time.

\vspace*{12pt}

\textbf{4. Set the gradient equal to zero. The result is known as the
\textit{normal equations}. Isolate $\beta$ on one side using the matrix inverse.}

\vspace*{12pt}

Setting the gradient equal to zero yields:
\begin{align}
2 X^t X \beta &= 2 X^t y.
\end{align}
And solving for $\beta$ gives:
\begin{align}
\beta &= (X^t X)^{-1} X^t y.
\end{align}
Assuming, of course, that the inverse exists.


\vspace*{12pt}

\textbf{5. Now, with a known quantity for $\beta$, write down an equation for $\widehat{y}$.
This should take the form:}
\begin{align}
\widehat{y} &= H y
\end{align}
\textbf{For some matrix $H$. The matrix here is called the ``hat'' matrix because it
puts a hat on the quantities of y. Show that $H^2 = HH = H$.}

\vspace*{12pt}

The equation for $\widehat{y}$ is given by:
\begin{align}
\widehat{y} &= X\beta\\
&= X (X^t X)^{-1} X^t y.
\end{align}
So that the hat matrix is:
\begin{align}
H &= X (X^t X)^{-1} X^t.
\end{align}
Showing that $H^2 = H$ is just a matter of expanding and cancelling terms:
\begin{align}
H^2 &= X (X^t X)^{-1} X^t X (X^t X)^{-1} X^t \\
&= X (X^t X)^{-1} (X^t X) (X^t X)^{-1} X^t \\
&= X (X^t X)^{-1} X^t \\
&= H.
\end{align}

\vspace*{12pt}

\textbf{6. Assume that the ``true'' value of $y$ is given by:}
\begin{align}
y &= X b + \epsilon
\end{align}
\textbf{For some vector of random errors $\epsilon$. Plug this into your equation for
$\beta$ and show that $\beta$ can be written as $b$ plus another term that should
be small if the noise terms are small.}

\vspace*{12pt}

Here we have:
\begin{align}
\beta &= (X^t X)^{-1} X^t y \\
&= (X^t X)^{-1} X^t (X b + \epsilon \\
&= (X^t X)^{-1} X^t X b + (X^t X)^{-1} X^t \epsilon \\
&= b + (X^t X)^{-1} X^t \epsilon.
\end{align}

\vspace*{12pt}

\textbf{7. Show that the residuals ($y - X\beta$) are perpendicular to the fitted
values $\widehat{y}$. That is, show that the dot product between the two is
zero. (Note: you can use the fact that $(X^tX)^{-1}$ is equal to its own transpose).}

\vspace*{12pt}

Notice that:
\begin{align}
\beta^t &= \left( (X^t X)^{-1} X^t y \right)^t \\
&= y^t X ((X^t X)^{-1})^t \\
&= y^t X (X^t X)^{-1}
\end{align}
Then, plugging in, everything eventually cancels:
\begin{align}
(y - X \beta)^t (X \beta) &= y^t X \beta - \beta^t X^t X \beta \\
&= y^t X (X^t X)^{-1} X^t y - X (X^t X)^{-1} X^t X (X^t X)^{-1} X^t \\
&= y^t X (X^t X)^{-1} X^t y - X (X^t X)^{-1} X^t \\
&= 0
\end{align}

\vspace*{12pt}

\textbf{8. Continue by following the notes in the \texttt{class04.Rmd}}

\vspace*{12pt}




\end{document}

