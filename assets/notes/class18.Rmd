---
title: "Class 18: Regularizing and Training Dense Neural Networks"
author: "Taylor Arnold"
output:
  html_document:
    css: "style.css"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(fig.path = "class18/")
knitr::opts_chunk$set(fig.height = 5)
knitr::opts_chunk$set(fig.width = 8.5)
knitr::opts_chunk$set(out.width = "100%")
knitr::opts_chunk$set(dpi = 300)
```

```{r, message = FALSE, warning = FALSE}
reticulate::use_python("/anaconda3/bin/python")

library(readr)
library(ggplot2)
library(dplyr)
library(methods)
library(keras)
theme_set(theme_minimal())
```

### MNIST, once again

You'll work with a different dataset for the lab, but let's use MNIST once
more for today's notes:

```{r}
mnist <- read_csv("https://statsmaths.github.io/ml_data/mnist_10.csv")
x28 <- read_rds("image_data/mnist_10_x28.rds")

X <- t(apply(x28, 1, cbind))
y <- mnist$class

X_train <- X[mnist$train_id == "train",]
y_train <- to_categorical(y[mnist$train_id == "train"], num_classes = 10)
```

Again, the task is to classify the image as belonging to one of 10 digits
(0-9).

### Neural Network Regularization

With MNIST, a small neural network goes a long way. The real power of
neural network though begin to come into play when working with larger
and deeping networks. We have already seen that it is easy to build
these with the **keras** package. For example, here is a model with four
hidden layers:

```{r}
model <- keras_model_sequential()
model %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal",
              input_shape = c(28^2)) %>%
  layer_activation(activation = "relu") %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal") %>%
  layer_activation(activation = "relu") %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal") %>%
  layer_activation(activation = "relu") %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal") %>%
  layer_activation(activation = "relu") %>%

  layer_dense(units = 10) %>%
  layer_activation(activation = "softmax")

model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_sgd(lr = 0.01),
                  metrics = c('accuracy'))

model
```

The new model now has over 1 million parameters. However, if we have too many parameters,
the model will begin to overfit.

```{r}
history <- model %>%
  fit(X_train, y_train, epochs = 16,
      validation_split = 0.1, batch_size = 32)
plot(history)
```

We need to regularize the model much like we did with the elastic net.
One way to regularize a neural network is to include a *dropout* layer.
This layer, during training only, randomly converts a proportion of its
inputs to zero. This forces the next layer to smooth out weights over
all of the inputs in the prior layer. It is similar to when random
forests only allow a random subset of the variable to be used at a
given branching point or how the weights in a ridge regression force
the model to spread out over correlated values.

Dropout can be added to the model with the `layer_dropout` function.
Here, we also change the initializing function for the starting
weights B in the neural network. I find that "glorot_normal" often
performs better when using dropout.

```{r}
model <- keras_model_sequential()
model %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal",
              input_shape = c(28^2)) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal") %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal") %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal") %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(units = 10) %>%
  layer_activation(activation = "softmax")

model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_sgd(lr = 0.01),
                  metrics = c('accuracy'))

model
```

We can train it just as with the simpler neural networks.

```{r}
history <- model %>%
  fit(X_train, y_train, epochs = 20,
      validation_split = 0.1, batch_size = 32)
plot(history)
```

Why is the training accuracy lower than the validation accuracy, particularly
for the first few epochs? The reason is that the training accuracy uses the
dropout functions but the validation accuracy turns the dropout off.

## Neural network training algorithm

From the plot, it looks like the model is not improving much after about 10
iterations through the dataset. It is possible to train the model with a
smaller learning rate to get a slightly better performance. Note that training
the model again without redefining the model will start where the previous
model left off.

```{r}
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_sgd(lr = 0.003),
                  metrics = c('accuracy'))
history <- model %>%
  fit(X_train, y_train, epochs = 2,
      validation_split = 0.1, batch_size = 32)
plot(history)
```

Another approach is to modify the SGD algorithm itself. Consider the standard
updates in gradient descent:

$$ w_{new} \leftarrow w_{old} - \rho \cdot \nabla_w f $$

A common alternative stores an additional momentum vector:

$$ \Delta w_{new} \leftarrow \mu \cdot \Delta w_{old} - \rho \cdot \nabla_w f $$
$$ w_{new} \leftarrow w_{old} + \Delta w_{new} $$

This allows us to approximate the Hessian matrix while only storing an extra p
variables (rather than p squared). To implement in R we can do this:

```{r}
model <- keras_model_sequential()
model %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal",
              input_shape = c(28^2)) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal") %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal") %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal") %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(units = 10) %>%
  layer_activation(activation = "softmax")

model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_sgd(lr = 0.01, momentum = 0.9,
                                            nesterov = TRUE),
                  metrics = c('accuracy'))
history <- model %>%
  fit(X_train, y_train, epochs = 2,
      validation_split = 0.1, batch_size = 32)
plot(history)
```

Finally, can we generalize the approach of changing the learning rate,
slowly making the learning rate smaller as we become stuck? Yes! Just use
a different optimizer. The RMSprop updates the learning rate as
a function of the improvement in each step:

```{r}
model <- keras_model_sequential()
model %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal",
              input_shape = c(28^2)) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal") %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal") %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(units = 512, kernel_initializer = "glorot_normal") %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(units = 10) %>%
  layer_activation(activation = "softmax")

model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(lr = 0.001),
                  metrics = c('accuracy'))
history <- model %>%
  fit(X_train, y_train, epochs = 10,
      validation_split = 0.1, batch_size = 32)
```

I find RMSprop to be OK on dense neural networks, but not as strong as
SGD with momentum.

### Tuning Neural Networks

The next lab has you take your turn at building a neural
network on a dataset very similar to MNIST. There are a
number of parameters that you can change in the neural
network. Here are the general rules
I would follow at this point:

- run a model for how ever many epochs (iterations) it takes
to visually flatline
- always build layers by putting together a `layer_dense`,
`layer_activation` ("relu" on all but the last, which should
be "softmax") and `layer_dropout`
- use `optimizer_sgd` with `nesterov = TRUE` and momentum
equal to `0.9`
- make all hidden layers have the same number of nodes
- start with 2 hidden layers with 128 nodes each and an
`lr = 0.01`

Then, you can change the following parameters as you test
which ones work the best. Make sure that you only change one
thing at a time and run enough epochs to get the model to
roughly converge

- experiment with between 2 and 5 hidden layers
- try doubling or halving the number of hidden nodes;
usually work in powers of 2
- if the model is very noisy, you can decrease the dropout
to 0.25
- try successively halving the learning rate

Try to start small and work up slowly so you do not crash
R (or, at least, you don't crash R too frequently).
