---
title: "Class 21: Transfer Learning for Words"
author: "Taylor Arnold"
output:
  html_document:
    css: "style.css"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(fig.path = "class21/")
knitr::opts_chunk$set(fig.height = 5)
knitr::opts_chunk$set(fig.width = 8.5)
knitr::opts_chunk$set(out.width = "100%")
knitr::opts_chunk$set(dpi = 300)
```

```{r, message = FALSE, warning = FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(stringi)
library(tokenizers)
library(smodels)
library(keras)
library(Matrix)
library(methods)
```

## Embeddings

Out approach to working with textual data involved counting
the occurrence of words, characters, or sequences of either. These
were summarized in a term frequency matrix, which we typically then
fed into an elastic net given the high-dimensionality of the dataset.
What subtleties are lost here?

- word placement in the text
- word forms
- negation
- context
- semantic meaning

How can we use a neural network to solve this problem? The output structure
is easy, and matches the one-hot encoding of the image processing tasks.
(from the idea that just one of the bytes is `hot', i.e. turned on.)
What about the input layer though? How do we feed a block of text into a
neural network?

Let's first simplify the problem and think about just a single word at a
time. How can we represent even a single word as an input to a neural
network? One approach is to determine a *vocabulary* of terms, these
are all of the words that we want to support in the classification task.
Usually we construct this by looking at all of the snippets of text and
taking the N-most commonly occurring words. Any words in the texts not
in this vocabulary are removed before further training. This is the same
thing we did with term frequency matrices, but now we are considering
just a single word.

Once we have this vocabulary, we can represent a single word by
an $N$-length binary vector with exactly one $1$:

$$ \text{apple} \rightarrow [0,0,0,\ldots,0,1,0,\ldots,0] $$

This is just another one-hot representation.

Suppose we use a one-hot representation as the first layer in a neural
network. If this is followed directly by a dense layer with p hidden
neurons, the weights in the layer can be defined as an N-by-p matrix
W. In this special case we do not need a bias term, because we already
know the scale of the previous layer (0's and 1's).

For a given set of weights W, because of the one-hot representation, the
values of the outputs from the first hidden layer will simply be row j
of the matrix W, where j is the index of the input word in the vocabulary.

![](img/tikz40.png)

A word embedding is nothing more than a compression of a one-hot
representation and a dense hidden layer in a neural network. There
is no need to actually create the one-hot vector, and multiply by
all of W. We can just go directly from the index of the word in
the vocabulary, and read off of the j-th row of W.

What are we doing here, really? The whole idea is to map a word as
a vector in a p-dimensional space:

$$ f(\text{word}) \rightarrow \mathbb{R}^p $$

We can have our neural network *learn* this word embedding by
giving it a large supervised learning model. If we do this on
one dataset and then use the embedding on a new model, this is
very similar to the transfer learning we did with iamges.

This is great, but most of the time we want to work with
a collection of words (a document) all as one entity. A
simple way to do this is to apply the word embedding to
each term, and the collapse (flatten) these into a single
long vector.

So if we have T terms in a document and a word embedding
with p terms, the output from the embedding layer will be
of size T times p. To be clear, the embedding step is agnostic
to the position of the word, much like the shared weights in a
convolutional neural network. The word "apple" is matched to
the same vector regardless of where in the sentence it is
found.

## Transfer learning of embeddings

One of the most power features of using neural networks for image
processing was the ability to use transfer learning. This is also
the case for working with word embeddings.

Unlike the CNN models, there are no pre-trained word embeddings
in **keras**. We need to use a seperate package to compute these
embeddings. One package is my own **fasttextM**. It is particularly
nice because it allows for doing word embeddings in a number of
languages into a common space. That is, we would expect that:

$$ || f_{EN} (cheese) - f_{FR} (fromage) || \leq \epsilon $$

For some relatively small value of epsilon. You can download models
using the function `ft_download_model` (this takes a while but needs
to be done only once):

```{r, eval = FALSE}
# devtools::install_github("statsmaths/fasttextM")

library(fasttextM)
ft_download_model("en")
ft_download_model("fr")
ft_download_model("zh")
```

Then, load each model you want to work with using `ft_load_model`:

```{r}
library(fasttextM)
ft_load_model("en")
ft_load_model("fr")
```

Finally, the function `ft_embed` takes a vector of words and returns
a 300 column matrix giving the embedding of each term:

```{r}
dim(ft_embed(c("horse", "dog", "cow")))
```


Several functions exist for understanding the structure of the word
embedding. The function `ft_nn` gives the nearest neighbor terms for
each input:

```{r}
ft_nn(c("cheese", "dog", "cow", "statistics", "apple", "London"))
```

If another language model is loaded, you can look up the nearest
neighbors in the other language:

```{r}
ft_nn(c("cheese", "dog", "cow", "statistics", "apple", "London"),
      lang_out = "fr")
```

Here we see that the model generally maps to translations and other
similar terms. I know we have a large number of students from China
or majoring in Chinese. Perhaps you can help tell me if this makes
any sense:

```{r}
ft_load_model("zh")
ft_nn(c("cheese", "dog", "cow", "statistics", "apple", "London"),
      lang_out = "zh")
```

My quick google search for ç‹—ç†Š showed found this bear, so I assume
the translation is not perfect, but is at least generally mapping
animals to animals.

![](http://a3.att.hudong.com/77/67/01300000164646121075676652916.gif)

## Applying embeddings to a supervised learning task

Finally, let's apply this algorithm to the Amazon product classification dataset.
The input dataset is the exact same as before.

```{r}
amazon <- read_csv("https://statsmaths.github.com/ml_data/amazon_product_class.csv")
amazon <- amazon[stri_length(amazon$text) > 100,]
words <- tokenize_words(amazon$text)
vocab <- top_n(count(data_frame(word = unlist(words)), word),
                n = 5000)$word
id <- lapply(words, function(v) match(v, vocab))
id <- lapply(id, function(v) v[!is.na(v)])
X <- pad_sequences(id, maxlen = 100)
y <- amazon$category
```

We can next apply the pre-trained word embedding to our Amazon product
data. This code embeds all of the available terms that we have, setting
missing terms to zero:

```{r}
X_words <- matrix(c("", vocab)[X + 1], nrow = nrow(X))
X_embed <- ft_embed(as.vector(X_words), lang = "en")
X_embed[is.na(X_embed)] <- 0
X_embed <- array(X_embed, dim = c(nrow(X_words), ncol(X_words), 100))
X_embed[1:2,,1]
```

And, using these embeddings, construct the training and validation datasets:

```{r}
X_train <- X_embed[amazon$train_id == "train",,]
X_valid <- X_embed[amazon$train_id == "valid",,]
y_train <- to_categorical(y[amazon$train_id == "train"] - 1, num_classes = 3)
y_valid <- to_categorical(y[amazon$train_id == "valid"] - 1, num_classes = 3)
```

I'll constuct a neural network model from this dataset. It is possible to use a
dense model or a 1D-convolution; I'm going to use a third type that we did not have
time to see this semester ðŸ˜•.

```{r}
model <- keras_model_sequential()
model %>%
  layer_lstm(units = 64,
             dropout = 0.2,
             recurrent_dropout = 0.2,
             return_sequences = FALSE,
             input_shape = dim(X_embed)[-1]) %>%
  layer_dense(units = 3, activation = 'softmax')
model

```

Once again, we compile the model and train:

```{r}
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = 'adam',
                  metrics = c('accuracy'))

history <- model %>% fit(X_train, y_train,
              batch_size = 32,
              epochs = 10,
              validation_data = list(X_valid, y_valid))
plot(history)
```

The model now has the best prediction rate of all our models:

```{r}
amazon$pred_category <- predict_classes(model, X_embed) + 1
tapply(amazon$pred_category == amazon$category,
       amazon$train_id, mean)
```

We could probably do even better by increasing the number of terms
in the sequences and not filtering out words from the top 5000
(I did the latter to simplify the code, but with the fasttext
word embedding there is no need).
