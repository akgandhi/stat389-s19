---
title: "Class 20: Classifying complex images with transfer learning"
author: "Taylor Arnold"
output:
  html_document:
    css: "style.css"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(fig.path = "class20/")
knitr::opts_chunk$set(fig.height = 5)
knitr::opts_chunk$set(fig.width = 8.5)
knitr::opts_chunk$set(out.width = "100%")
knitr::opts_chunk$set(dpi = 300)
```

```{r, message = FALSE, warning = FALSE}
library(readr)
library(ggplot2)
library(dplyr)
library(methods)
library(stringi)
library(keras)
```

## Classifying complex image - Transfer learning

We have already seen several examples of classifying image corpora. When
detecting whether our class photographs were taken inside or outside, the
algorithm primarily used color to detect the brightness and spectrum to
see if it looked like sunlight (more blue) or artificial light (more red).
With the flowers dataset we used color as well as texture to figure out
what type of flower was shown in the image. With the MNIST, Fashion MNIST,
and EMNIST datasets neural networks were used to detect and classify
more complex features such as shape and location in the image.

In the world of image classification, all of these tasks are fairly easy.
The first few only needed average values of pixels at a very low resolution.
The latter ones were zoomed in, centered, and standardized in size and
orientation. Most classification tasks out in the wild are much more complex.

How would we go about solving these types of problems? The simple answer is
that you need millions of images, a very deep and large convolutional
neural network, and a large computing cluster with access to many
expensive GPUs.

The good news is that there is a solution for all of us that do not have
access to such large datasets. Recall that a convolutional neural network
usually consists of several convolutional layers followed by a flattening
operation and dense layers. It turns out that if a deep neural network
is trained with a large corpus of images, the lower convolutional layers
pick out features that can be used for other image processing tasks. We
can actually think of the CNN layers as doing feature extraction, much
like we did with the textual data when counting tokens and character
shingles.

The reason this is so helpful is that many groups (those with access to
large computing resources) have published their trained models. We can
fit just lower levels of these models to new training data, and then use
the output of this in our own models. These could be more neural networks,
but there is nothing stopping us from using a different model on the
learned features. This process is called *transfer learning*. It
is one of the most important features of neural networks. Otherwise it
would only be possible for a limited number of research groups to make
real use of these techniques.

Today, we will apply transfer learning to two tasks: face detection in
our class dataset and detecting whether an image contains a dog or a
cat.

## ResNet50

The **keras** package provides several pre-trained models for us to
use. At the current moment there are:

- `application_inception_v3`:  Szegedy, Christian, et al. "Rethinking the inception architecture for computer vision." *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.* 2016.
- `application_vgg16`: Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014).
- `application_vgg19`: Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014).
- `application_mobilenet`: Howard, Andrew G., et al. "Mobilenets: Efficient convolutional neural networks for mobile vision applications." arXiv preprint arXiv:1704.04861 (2017).
- `application_resnet50`: He, Kaiming, et al. "Deep residual learning for image recognition." *Proceedings of the IEEE conference on computer vision and pattern recognition.* 2016.
- `application_xception`: Chollet, Fran√ßois. "Xception: Deep Learning with Depthwise Separable Convolutions." arXiv preprint arXiv:1610.02357 (2016).

These were all trained on the same corpus from the **ImageNet Challenge**,
which includes images from 1000 categories all scaled to 224 by 224 pixels.
The images look something like this:

![](http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k.jpg)

For more information about the corpus, see the paper here:

- Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei.  *ImageNet Large Scale Visual Recognition Challenge. IJCV,* 2015.

We are going to use the ResNet50 model. The 50 refers to their being 50
trainable layers in the neural network. Let's load the model, which includes
trained weights and look at the architecture.

```{r}
resnet50 <- application_resnet50(weights = 'imagenet', include_top = TRUE)
resnet50
```

Notice that there are over 25 million weights that need to trained in
this model. Most of them, however, are in the last two dense layers.
The convolutions can be grouped into five blocks, defined by how large
a subset (in pixels) the convolution is looking at. Each successive
convolution is grabbed larger and more complex features than the previous
one. Very roughly, the five blocks can be thought of as capturing the
following aspects of the image:

- 2x2: double convolution, gradients
- 4x4: double convolution, edges
- 8x8: triple convolution, shapes and textures
- 16x16: triple convolution, objects and object textures
- 32x32: triple convolution, objects in context and entities

We will fit the lower-level parts of the VGG16 model in order to use these
learned features to detect faces and dogs.

## Dogs versus cats

Next, lets look at a dataset of photos showing cats and dogs. This data is
not cleaned like the other data we have seen this semester. We just have two
folders of image data: one folder has images of dogs and the other has images
of cats.

Let's just read in a single image. The following code takes the image found in
the `image_path` location and converts it into an array of the type that we would
pass directly into keras:

```{r}
image_path <- "image_data/dog_cat/dog/dog.1.jpg"
image <- image_load(image_path, target_size = c(224,224))
image <- image_to_array(image)
image <- array_reshape(image, c(1, dim(image)))
dim(image)
```

Let's look at the image:

```{r}
par(mar = rep(0, 4L))
plot(0,0,xlim=c(0,1),ylim=c(0,1),axes= FALSE, type = "n", asp=1)
rasterImage(image[1,,,] / 255,0,0,1,1)
```

We want to pass this image through the ResNet50 model, but we do not want the final output
classification layer. Instead, we want to grab the second to last layer. To construct a model
that does this, use the following `keras` code:

```{r}
model_avg_pool <- keras_model(inputs = resnet50$input,
                              outputs = get_layer(resnet50, 'avg_pool')$output)
```

Now, before putting the image of our dog into this model, we need to do some preprocessing that
matches the processing the ResNet50 groud used on their corpus

```{r}
image_pp <- imagenet_preprocess_input(image)
```

Finally, we can predict using the model just as an other model that we would work with:

```{r}
pred <- predict(model_avg_pool, x = image)
dim(pred)
```

It returns one set (for the one image) of 2048 numbers. There is not much we can do with just
this one set. We need to repeat with the entire dataset.

### Embedding the corpus

Knowing that you all like to copy my code without thinking much about it, I'll try to write it here
in a way that will generalize to any input directory containing a set of images in different folders.

```{r}
input_dir <- "image_data/dog_cat"

image_paths <- dir(input_dir, recursive = TRUE)
ext <- stri_match(image_paths, regex = "\\.([A-Za-z]+$)")[,2]
image_paths <- image_paths[stri_trans_tolower(ext) %in% c("jpg", "png", "jpeg")]
class_vector <- dirname(image_paths)
class_names <- levels(factor(class_vector))

n <- length(class_vector)
Z <- array(0, dim = c(n, 224, 224, 3))
y <- as.numeric(factor(class_vector)) - 1L
for (i in seq_len(n))
{
  pt <- file.path(input_dir, image_paths[i])
  image <- image_to_array(image_load(pt, target_size = c(224,224)))
  Z[i,,,] <- array_reshape(image, c(1, dim(image)))
}
```

Now we have a fairly decent dataset to work with. It is a good idea to randomly permute the
dataset.

```{r}
set.seed(1)
index <- sample(seq_len(nrow(Z)))
Z <- Z[index,,,]
y <- y[index]
```

Next, let's do the embedding (this takes a few minutes)

```{r}
X <- predict(model_avg_pool, x = imagenet_preprocess_input(Z), verbose = TRUE)
dim(X)
```

With the new embedding matrix, let's construct a training dataset. Here I am using
a 60/40 split, but you can always modify this.

```{r}
train_id <- sample(c("train", "valid"), nrow(X), TRUE, prob = c(0.6, 0.4))

X_train <- X[train_id == "train",]                  # Note: X is a matrix
y_train <- to_categorical(y[train_id == "train"])
```

With this dataset, we can fit any model that we want, though its easy enough to just
use a neural network:

```{r}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 256, input_shape = ncol(X_train)) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(units = 256) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(units = ncol(y_train)) %>%
  layer_activation(activation = "softmax")

model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(lr = 0.001 / 2),
                  metrics = c('accuracy'))

history <- model %>%
  fit(X_train, y_train, epochs = 8)
plot(history)
```

How well does this model make predictions? It almost perfectly fits the training set
end gets over 98% correct on the test set, this with a fairly complex task and only a
few hundred training examples.

```{r}
y_pred <- predict_classes(model, X)
tapply(y == y_pred, train_id, mean)
```

With two classes, the confusion matrix is not very interesting, but it is useful in
other cases:

```{r}
table(value = class_names[y + 1L], prediction = class_names[y_pred + 1L], train_id)
```

We can also look at some negative examples, but there really are not many here:

```{r}
par(mfrow = c(2, 3))
id <- which(y_pred != y)
for (i in id) {
  par(mar = rep(0, 4L))
  plot(0,0,xlim=c(0,1),ylim=c(0,1),axes= FALSE,type = "n")
  rasterImage(Z[i,,,] /255,0,0,1,1)
  text(0.5, 0.1, label = class_names[y[i] + 1L], col = "red", cex=6)
}
```

Can you see why these might be difficult to classify?

Finally, we can also find those examples that have the highest probability of being
in a class. First, get all of the probabilities:

```{r}
y_probs <- predict(model, X)
```

Then, this code gives the highest classification rate for types of :

```{r}
type <- "cat"

# which are the maximum probs?
id <- order(y_probs[,which(class_names == type)], decreasing = TRUE)[1:15]

par(mfrow = c(3, 5))
for (i in id) {
  par(mar = rep(0, 4L))
  plot(0,0,xlim=c(0,1),ylim=c(0,1),axes= FALSE,type = "n", asp=1)
  rasterImage(Z[i,,,] /255,0,0,1,1)
  text(0.5, 0.1, label = class_names[y[i] + 1L], col = "red", cex=6)
}
```

Then, the same code for the dogs

```{r}
type <- "dog"

# which are the maximum probs?
id <- order(y_probs[,which(class_names == type)], decreasing = TRUE)[1:15]

par(mfrow = c(3, 5))
for (i in id) {
  par(mar = rep(0, 4L))
  plot(0,0,xlim=c(0,1),ylim=c(0,1),axes= FALSE,type = "n", asp=1)
  rasterImage(Z[i,,,] /255,0,0,1,1)
  text(0.5, 0.1, label = class_names[y[i] + 1L], col = "red", cex=6)
}
```

Does it make sense to you that these are the most cat-like cats and dog-like dogs?

### A bit of visualization

I can't resist showing off one last thing. Let's try to visualize the embedding itself
using principle components:

```{r}
pca <- as_tibble(prcomp(X)$x[,1:2])
pca$y <- class_names[y + 1L]
```

And plot it:

```{r}
ggplot(pca, aes(PC1, PC2)) +
  geom_point(aes(color = y), alpha = 0.2, size = 7) +
  labs(x = "", y = "", color = "class") +
  theme_minimal()
```




