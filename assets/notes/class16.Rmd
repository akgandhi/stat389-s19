---
title: 'Class 16: Classification with Neural Networks'
author: "Taylor Arnold"
output:
  html_document:
    css: "style.css"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(fig.path = "class16/")
knitr::opts_chunk$set(fig.height = 5)
knitr::opts_chunk$set(fig.width = 8.5)
knitr::opts_chunk$set(out.width = "100%")
knitr::opts_chunk$set(dpi = 300)

options(width=75)
```

```{r, message = FALSE}
library(readr)
library(ggplot2)
library(dplyr)
library(keras)
```

## Classification with neural networks

We have just seen how to use neural networks to do regression with one
continuous response. Of course, we could modify this to do binary
classification by coding factors as 0 and 1. Further, the *one-vs-one*
and *one-vs-many* paradigms would allow this to extend to multiclass
classification. There is, however, a more straightforward approach.
If you recall that we used the **nnet** package to fit the multinomial
function, this should not be surprising.

### One-hot encoding

The trick for doing multiclass regression with neural networks is to
realize that we can easily make the last layer of a neural network
output multiple values. All we need to do is have the function that
we want to minimize by a function of all these outputs rather than
a single output with each row of the data. An easy way to do this is
to assign multiple response variables with each observation and do
mean squared error loss, but we will now take the mean squared loss of
predicting all the outputs.

The **keras** package include a function `to_categorical` that converts
numeric class labels to binary indicator variables. This is very similar
to the model matricies we built for the X matrix when using categorical
predictors. Here is a simple example of its application:

```{r}
to_categorical(c(1,1,2,4,10))
```

Notice that keras is 0-indexed, so it wants the first category to be zero.
That's why we have 11 columns even thought the largest category is ten.
The best way to understand how to use this response matrix is to see
a worked example in keras.

### Chicago Crimes, again

I am going to load another version of the Chicago crimes data, but this
time there are 12 crime types:

```{r, message = FALSE}
crimes <- read_csv("https://statsmaths.github.io/ml_data/chi_crimes_12.csv")
```

I want to just use latitude, longitude, and hour here to simplify the analysis.
The X data matrix is unchanged from previous code examples other than the
fact that we scale the columns.

```{r}
X <- scale(as.matrix(select(crimes, longitude, latitude, hour)))
X_train <- X[crimes$train_id == "train",]
X_valid <- X[crimes$train_id == "valid",]
```

Typically, I create a y variable as before, but make sure to subtract one so that
the smallest category is zero. When creating `y_train` and `y_valid`, I use the
`to_categorical` function, manually specifying the number of classes to avoid any
issues:

```{r}
y <- crimes$crime_type - 1
y_train <- to_categorical(y[crimes$train_id == "train"], num_classes = 12)
y_valid <- to_categorical(y[crimes$train_id == "valid"], num_classes = 12)
```

Take note of what the response matrix looks like:

```{r}
y_train[sample(nrow(y_train), 10),]
```

### Building a classification model

Here is the specificiation of a model that could train on our dataset. The
input shape is three because we have three input variables. The last layer
has 12 units because the respones has 12 columns. I use a new activation layer
at the end called a "softmax". The forces the output values to all be
positive and sum to 1; therefore we can safely view them as proper
probabilities.

```{r}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 20, input_shape = c(3)) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 20) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 12) %>%
  layer_activation(activation = "softmax")
model
```

Notice that this fairly simple model already has 752 parameters that we need
to learn. Neural networks can get complex very quickly!

Now, we compile the model. While "mse" could work for the function we want to
minimize, something called "categorical_crossentropy" is better for some technical
reasons when learning to do multiclass classification. We'll also print out the
accuracy at each step.

```{r}
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(lr = 0.01),
                  metrics = c('accuracy'))
```

```{r, message = FALSE, warning = FALSE}
model %>%
  fit(X_train, y_train, epochs = 10,
      validation_data = list(X_valid, y_valid))
```

Random guessing yields a classification rate of 8%, so our model does approximately
twice as well as guessing. Not bad for a first take at using neural networks.

In order to get predictions on the whole dataset, we can use the function `predict_classes`.
A `predict` function exists, but it would give us a matrix of values much like `y_train`.
The `predict_classes` function helpfully saves a step. **Don't forget to add one to predictions!**

```{r}
crimes$crime_type_pred <- predict_classes(model, x = X) + 1
```

Finally, take a look at the lower right-hand panel in RStudio. You will see a helpful plot of
the training and validation loss functions metrics.


## Neural Network Internals

At this point you should have a good idea of what a neural
network is once we have learned the weight matricies and
bias vectors. We have not spent any time discussing exactly
how neural networks are actually trained. In short, they
use a modification of gradient descent. Recall that this is
an optimization technique where (for minimization) we take
a small step in the opposite direction of the gradient
vector of partial derivatives.

There are three tweaks / tricks that make this feasible for
neural networks:

- the use of stochastic gradient descent
- backpropagation
- momentum

Back propagation is a trick for computing the gradient very
quickly. Momentum is a way of keeping track of prior gradients
so that we avoid the pitfalls of ordinary gradient descent.
In short, the gradient in the current step is averaged with
some proportion of the prior gradient. This way, if we are
stuck in a flat area the gradients will accumulate and "pick
up speed". If we are bouncing over the sides of a valley, the
gradients cancel each other out and lower the step size. It
is a very clever technique that has huge performance benefits.
(See the notes from Class 04, "Fuel Efficiency in the Big City",
for a review of gradient descent and some of its pitfalls).

Stochastic gradient descent (SGD) is a way of incrementally updating
the weights in the model without needing to work with the entire
dataset at each step. The name is an anachronism, as the current
way of doing SGD does not involve any randomness. To understand
SGD, consider updates in ordinary gradient descent:e

$$ \left( w^{(0)} - \eta \cdot \nabla_w f \right) \, \rightarrow \, w^{(1)} $$

Notice that for squared error loss (it is also true for categorical
cross-entropy), the loss can be written as a sum of component losses
for each observation. The gradient, therefore, can also be written
as a sum of terms over all of the data points.

$$ \begin{align} f(w) &= \sum_i (\widehat{y}_i(w) - y_i)^2 \\
        &= \sum_i f_i(w) \\
        \nabla_w f &= \sum_i \nabla_w f_i
    \end{align}$$

This means that we could write gradient descent as a series of
n steps over each of the training observations.

$$
\begin{align}
\left( w^{(0)} - (\eta / n) \cdot \nabla_{w^{(0)}} f_1 \right) \, &\rightarrow \, w^{(1)} \\
\left( w^{(1)} - (\eta / n) \cdot \nabla_{w^{(0)}} f_2 \right) \, &\rightarrow \, w^{(2)} \\
&\vdots \\
\left( w^{(n-1)} - (\eta / n) \cdot \nabla_{w^{(0)}} f_n \right) \, &\rightarrow \, w^{(n)} \\
\end{align}
$$

The output $w^{(n)}$ here is exactly equivalent to the $w^{(1)}$ from before.
SGD actually does the updates in this iterative fashion, but makes one
modification. In each step it updates the gradient with respect to the
new set of weights. Writing eta prime as eta divided by the sample size,
we can write this as:

$$
\begin{align}
\left( w^{(0)} - \eta' \cdot \nabla_{w^{(0)}} f_1 \right) \, &\rightarrow \, w^{(1)} \\
\left( w^{(1)} - \eta' \cdot \nabla_{w^{(1)}} f_2 \right) \, &\rightarrow \, w^{(2)} \\
&\vdots \\
\left( w^{(n-1)} - \eta' \cdot \nabla_{w^{(n)}} f_n \right) \, &\rightarrow \, w^{(n)} \\
\end{align}
$$

The approach of SGD should seem quite reasonable. Why work with
old weights in each step when we already know what direction the
points are moving in? One pass through the entire dataset is
called n *epoch*.

Typically, **keras** and other neural network software implement a
batched version of SGD. Instead of splitting the data into
individual observations it instead buckets the data into small
groups known as *mini batches*. The entire mini batch is used
in each step of the algorithm. This is the `batch_size` parameter
you may have noticed in the code chunks. Generally the only
effect of this number is the speed of the algorithm and it
tends to be set to some small power of 2.

If you are interested even more in understanding the internals
of the backpropigation algorithm, the only part that we have
truly glossed over, I recommend the notes by Michael Nielsen:

- [http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)

This is just Chapter 2 of his free online text. The entire
digital book is fantastic and an excellent reference that goes
much deeper into the details than we are able to this semester.

