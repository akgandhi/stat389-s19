%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt,hidelinks]{article}

% 1. Load LaTeX packages
\usepackage{fontspec}
\usepackage{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xunicode}
\usepackage{listings}
\usepackage{color}
\usepackage{amssymb}

% 2. Define page dimensions and spacing
\geometry{top=1in, bottom=1in, left=1in, right=2in, marginparsep=4pt,
          marginparwidth=1in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

% 3. Set header, footer, and bibliography
\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancyplain}
\fancyhf{}
\lfoot{}
\rfoot{page \thepage\ of \pageref{LastPage}}
\bibliographystyle{acm}

% 4. Set fonts for the document
\defaultfontfeatures{Mapping=tex-text}
\setromanfont{YaleNew}

% 5. Define custom code for book environments and commands
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textbf{#1}}

% 6. Define custom code for book environments and commands
\definecolor{verbgray}{gray}{0.9}
\definecolor{verbgray2}{gray}{0.975}

\lstnewenvironment{rcode}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

\lstnewenvironment{rres}{%
  \lstset{backgroundcolor=\color{verbgray2},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

% 7. Define numbering scheme for equations (only needed for handout).
\numberwithin{equation}{section}
\setcounter{section}{6}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

{\LARGE Handout 06: Ridge Regression}

\vspace*{18pt}

On the last handout I asked to consider a regression problem with an estimated
value for $\beta$ and a data matrix $X$ factorized using the SVD as $UDV^{t}$.
Then, we considered the predictions from a new $\tilde{\beta}$ equal to $\beta$
plus a multiple of the smallest right singular vector ($V_p$). This is given
by:
\begin{align}
X(\tilde{\beta}) &= X(\beta + a V_p) \\
&= X\beta + a X V_p \\
&= X\beta + a \sigma_p.
\end{align}
In the lab questions, you assumed $\sigma_p=0$ and this shows that the predictions
$\widehat{y}$ for $\beta$ are exactly equivalent to the predictions for $\tilde{\beta}$.
What if $\sigma_p$ is positive but small? In this case the predictions are not
exactly the same but they are still very difficult to distinguish. Under sufficient
noise it is still nearly impossible to distinguish between these two solutions when
$\sigma_p$ is small. This can make regression very difficult to perform because
large datasets often have a smallest singular value that is quite small (more on
this later).

The fundamental problem here is that we are only the mean squared error as our
loss function. Therefore, there is no easy way to distinguish between using
$\beta$ and $\tilde{\beta}$. One solution is to modify the loss function to make
it easier to distinguish between these two solutions. For example, here is the
equation for ridge regression:
\begin{align}
\beta_{\lambda} &= \argmin_b \left\{ || y - Xb||_2^2 + \lambda ||b||_2^2 \right\}
\end{align}
For some constant $\lambda > 0$. It says that you want to minimize the errors
in prediction, but with an additional cost associated with large values of $\beta$.
This helps to distinguish between the many possible models and often does a much
better job than the ordinary least squares estimator at predicting future values.
You can derive a very elegant solution to the ridge regression, particular when
you incorporate the SVD. I will have you derive this in the following lab questions.

% \textbf{PCA}

% There is a closely related concept to ridge regression known as principal component
% analysis. It also comes quite cleanly from the SVD decomposition, but is not
% directly associated with predictive modelling. The question it attempts to answer
% is: How can we visualize a matrix $X$ when $p$ is large? The idea is to capture
% the variation in $X$ in a small number of dimensions; usually this is done in two
% dimensions so that we can easily plot the dataset.

% Consider the SVD of a matrix $X$:
% \begin{align}
% X &= U D V^t.
% \end{align}
% The most important terms, in terms of the overall size of the map $X$, are
% those corresponding with the largest singular values. To approximate this, we
% can remove all but the largest $k$ singular values from d. Furthermore, in
% visualization, the final rotation $V^t$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\textbf{LAB QUESTIONS}

\vspace*{0pt}

\begin{enumerate}
\item Take the gradient of the ridge regression loss function and set it equal
to zero. Get an equation for $\beta_{\lambda}$ in terms of $X$ and $y$. You may
need to use the fact that $b^t b$ is equal to $b^t I_p b$ in this derivation.
(Note: Do \textbf{not} yet use the SVD of $X$ here).
\item The eigenvalue decomposition of a matrix writes a matrix as $Q^t \Lambda Q$ for
a diagonal matrix $\Lambda$ (the entries are called the matrix eigenvalues) and an
orthogonal matrix $Q$. Unlike the SVD, the eigenvalue decomposition only applies to
square matrices and even then does not always exist. Show that the eigenvalues of
$X^t X$ are equal to the squared singular values of $X$.
\item I want to derive a formula for $\beta_{\lambda}$ in terms of the SVD of X;
this is not a long derivation but does require a trick. Take the equation that you
have already derived for $\beta_{\lambda}$; there should be an identity matrix in
the formula. consider the SVD of $X$ as $UDV^t$. Write the identify matrix in the
equation as $V^tV$, plug in the SVD for $X$, and simplify. You should be able to
factor out some of the terms and are left with something very similar to equation
we had for the ordinary leasts squares estimator.
\item Understand that the ridge regression is equivalent to fitting ordinary
least squares on a new matrix $\bar{X}$ where the singular values have been
increased by a factor of $\lambda$. Given our argument about the problems with
the smallest singular value, does it make sense that this change alleviates the
problem of identifiability?
\item Let's somewhat switch gears here and consider a specific example problem.
Let $p=2$ and assume that the first column of $X$ ($X_1$) can be written as:
\begin{align}
X_1 = \alpha + X_2, \quad \alpha \in \mathbb{R}^n
\end{align}
Where $\alpha$ is a small noise vector. So, $X_1$ and $X_2$ are very similar to
one another. Write an equation for the value $X b$, factoring in terms of $\alpha$
and $X_2$ (there should not be any $X_1$ left in the equation). Then, assume that
we have data generated by:
\begin{align}
y &= X_2 + \text{noise}
\end{align}
Where the noise is not too large. Convince yourself that all of the following
values of $b$ produce a reasonable estimate for $\widehat{y} = X b$:
\begin{align}
b &= \begin{bmatrix} 0 \\ 1 \end{bmatrix} \\
b &= \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\
b &= \begin{bmatrix} -1 \\ 2 \end{bmatrix} \\
b &= \begin{bmatrix} -100 \\ 101 \end{bmatrix}
\end{align}
What do you think is the approximate value of $\beta_\lambda$ for ridge
regression for a small value of $\lambda$ assuming the noise vector and $\alpha$
are also both small?
\item For the previous question, can you guess a plausible value for $V_p$
(the last left singular) of the matrix $X$? Look back at handout 5, question 3,
for a hint.
\item In the following final questions, we are going to consider a dataset
where:
\begin{align}
X^t X &= 1_p \cdot n.
\end{align}
There is nothing you need to compute here, but make sure that you understand:
(a) why it makes sense to include the $n$ on the right-hand side, and (b) why
it makes sense that we say in this case that the columns of $X$ are uncorrelated.
\item Taking $X^t X = 1_p \cdot n$, what is the value of the ordinary least
squares estimator? Can you explain exactly what a particular component $\beta_k$
is in terms of an inner product?
\item Again taking $X^t X = 1_p \cdot n$, write the ridge regression vector
$\beta_\lambda$ as a function of just $n$, $\lambda$, and the OLS solution $\beta$.
\end{enumerate}

\end{document}

