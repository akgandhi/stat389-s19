%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt,hidelinks]{article}

% 1. Load LaTeX packages
\usepackage{fontspec}
\usepackage{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xunicode}
\usepackage{listings}
\usepackage{color}
\usepackage{amssymb}

% 2. Define page dimensions and spacing
\geometry{top=1in, bottom=1in, left=1in, right=2in, marginparsep=4pt,
          marginparwidth=1in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

% 3. Set header, footer, and bibliography
\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancyplain}
\fancyhf{}
\lfoot{}
\rfoot{page \thepage\ of \pageref{LastPage}}
\bibliographystyle{acm}

% 4. Set fonts for the document
\defaultfontfeatures{Mapping=tex-text}
\setromanfont{YaleNew}

% 5. Define custom code for book environments and commands
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textbf{#1}}

% 6. Define custom code for book environments and commands
\definecolor{verbgray}{gray}{0.9}
\definecolor{verbgray2}{gray}{0.975}

\lstnewenvironment{rcode}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

\lstnewenvironment{rres}{%
  \lstset{backgroundcolor=\color{verbgray2},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

% 7. Define numbering scheme for equations (only needed for handout).
\numberwithin{equation}{section}
\setcounter{section}{5}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

{\LARGE Handout 05: Singular Value Decomposition}

\vspace*{18pt}

Today we are going to work with a particular type of matrix factorization
called the singular value decomposition. Start by assuming that we have a
matrix $A$ with $n$ rows and $p$ columns such that $n \geq p$. The (thin)
singular value decomposition, or SVD, is given by the matrix product:
\begin{align}
A &= U D V^t
\end{align}
With the following dimensions:
\begin{align}
A \in \mathbb{R}^{n \times p} \\
U \in \mathbb{R}^{n \times p} \\
D \in \mathbb{R}^{p \times p} \\
V \in \mathbb{R}^{p \times p}
\end{align}
Furthermore, $D$ is a diagonal matrix with non-negative entries along the
diagonal ordered from the largest to the smallest value:
\begin{align}
D &= \begin{bmatrix}
\sigma_1 & 0 & \cdots & 0\\
0 & \sigma_2 & \cdots & 0 \\
0 & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma_p \end{bmatrix}, \quad \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_p \geq 0.
\end{align}
The values $\sigma_k$ are called the \textit{singular values} of the matrix $A$.
Also, $V$ is an orthogonal matrix such that (we showed in Handout 03 that this
corresponds to a rotation):
\begin{align}
V^t V &= V V^t = I_p.
\end{align}
The matrix $U$ is not square, so it cannot be completely orthogonal, but its
columns are orthogonal to one another so we have:
\begin{align}
U^t U &= I_p.
\end{align}
The singular value decomposition exists for any matrix, and so we can use it
without any assumptions on the matrix we are working with. This has important
geometric implications: \textbf{any} linear function can be written as a
rotation, a fixed scaling of the components, and another rotation.

\textbf{SVD and the Normal Equations}

If we take the SVD of the data matrix $X$, we have
\begin{align}
X &= U D V^t.
\end{align}
Plugging this into the ordinary least squares estimator gives:
\begin{align}
\beta &= (X^t X)^{-1} X^t y \\
&= (V D^t U^t U D V^t)^{-1} V D^t U^t y \\
&= (V D (U^t U) D V^t)^{-1} V D U^t y \\
&= (V D I_p D V^t)^{-1} V D U^t y \\
&= (V D^2 V^t)^{-1} V D U^t y
\end{align}
By taking the fact that a diagonal matrix is its own transpose and using that
$U^t U$ is equal to the identity. Note that $D^2$ is just a matrix with the
squared singular values along the diagonal.

Now, notice that the inverse of $V$ is $V^t$, and vice-versa. Further, the
inverse of $D^{2}$ is equal to a diagonal matrix with the inverse of the
squared singular values along the diagonal (this exists if we assume that
$\sigma_1 > 0$). Therefore:
\begin{align}
(V D^2 V^t)^{-1} &= (V^{t})^{-1} D^{-2} V^{-1} = V D^{-2} V^t
\end{align}
And we can further simplify the equation for the ordinary least squares
estimator:
\begin{align}
\beta &= (V D^2 V^t)^{-1} V D U^t y \\
&= V D^{-2} V^t V D U^t y \\
&= V D^{-2} D U^t y \\
&= V D^{-1} U^t y.
\end{align}
This gives us a compact way to write the ordinary least squares estimator.
It is also far more numerically stable to use this formula to compute the
estimate $\beta$ from a dataset. Most importantly, it will yield a lot of
intuition for what makes some estimation tasks hard and motivate how we can
(partially) address the most challenging regression problems.

\textbf{SVD in R}

In R, you can create the singular value decomposition of a matrix using the
function \texttt{svd}. To see this, let's construct some simulated data:
\begin{rcode}
set.seed(1)
n <- 1e4; p <- 4
X <- matrix(rnorm(n*p), ncol = p)
b <- c(1,2,3,4)
epsilon <- rnorm(n)
y <- X %*% b + epsilon
\end{rcode}
Now, we take the singular value decomposition of the matrix. I will also
explicitly extract out and save the matrices $U$ and $V$ as well as the singular
values $sigma$:
\begin{rcode}
svd_output <- svd(X)
U <- svd_output[["u"]]
V <- svd_output[["v"]]
sigma <- svd_output[["d"]]
\end{rcode}
Now, lets compute the ordinary least square matrix with this data:
\begin{rcode}
beta <- V %*% diag(1 / sigma) %*% t(U) %*% y
beta
\end{rcode}
\begin{rres}
          [,1]
[1,] 0.9870134
[2,] 1.9876739
[3,] 3.0045489
[4,] 4.0102080
\end{rres}
We can verify that this is equivalent to our old form of the estimator by:
\begin{rcode}
solve(t(X) %*% X) %*% t(X) %*% y
\end{rcode}
\begin{rres}
          [,1]
[1,] 0.9870134
[2,] 1.9876739
[3,] 3.0045489
[4,] 4.0102080
\end{rres}
Notice that both are close to the value of \texttt{b} in the simulation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\textbf{LAB QUESTIONS}

\vspace*{0pt}

\begin{enumerate}
\item I showed you how to get a nice equation for $\beta$ in the ordinary
least squares equation. Using the SVD of $X$, compute a compact formula for
the values $\widehat{y} = X \beta$.
\item We glossed over the case where one or more of the singular values is equal
to zero. In this question I will show you why we cannot deal with this case
in the construction of $\beta$. Let $V_p$ denote the last column of $V$ (these
columns are called the \textit{right singular vectors}). Argue that:
\begin{align}
V^t V_p = \begin{bmatrix} 0 \\ 0 \\ \cdots \\ 0 \\ 1 \end{bmatrix}
\end{align}
Now, assume that $\sigma_p = 0$. Show that (Hint: expand X with the SVD):
\begin{align}
XV_p = 0.
\end{align}
\item Continuing with the last problem, assume that we have a potential
candidate $\beta$ for the regression vector. Show that the fitted values
$\widehat{y}$:
\begin{align}
\widehat{y} &= X \beta = X (\beta + a \cdot V_p), \quad \forall a \in \mathbb{R}.
\end{align}
Explain why this implies that we cannot uniquely determine a value for $\beta$
according the minimization of the loss function on the training data when
$\sigma_p = 0$.
\item Let $X$ be a matrix with SVD equal to $UDV^t$ and $w$ be a
$p$-dimensional vector with Euclidean norm equal to one:
\begin{align}
|| w ||_2^2 &= w^t w = \sum_k w_k^2 = 1.
\end{align}
It is generally true that we can write the vector $w$ as a weighted sum of
the columns of $V$:
\begin{align}
w &= \sum_k a_k \cdot V_k.
\end{align}
I want you to show that $\sum_k a_k^2 = 1$. This is straightforward
\textit{assuming that you approach the problem in a particular way}. Start
by writing out $|| w ||_2^2$ as an inner product and expanding in the basis
of $V$:
\begin{align}
1 = || w ||_2^2 = w^t w &= \left(\sum_k a_k \cdot V_k \right)^t \left(\sum_k a_k \cdot V_k \right) \\
&= \left(\sum_k a_k \cdot V_k^t \right) \left(\sum_k a_k \cdot V_k \right)
\end{align}
Then, take the cross terms to write this as a double sum and simplify the
result.
\item Let $X$ be a matrix with SVD equal to $UDV^t$ and $w$ be a
$p$-dimensional vector. Show that:
\begin{align}
|| X w ||_2^2 &= || D V^t w ||_2^2
\end{align}
In other words, the matrix $U$ does not effect the size of the product $X w$.
\item Let $X$ be a matrix with SVD equal to $UDV^t$ and $w$ be a
$p$-dimensional vector that we will write as:
\begin{align}
w &= \sum_k a_k \cdot V_k.
\end{align}
Show that:
\begin{align}
V^t w &= \begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_p \end{bmatrix}.
\end{align}
And therefore, we have:
\begin{align}
D V^t w &= \begin{bmatrix} \sigma_1 \cdot a_1 \\ \sigma_2 \cdot a_2 \\ \vdots \\ \sigma_p \cdot a_p \end{bmatrix}
\end{align}
\item Using the set-up from the previous question, show that:
\begin{align}
|| X w ||_2^2 &= \sum_k a_k^2 \sigma_k^2.
\end{align}
\item Let $X$ be a matrix with SVD equal to $UDV^t$. Consider the $\ell_2$-ball
given by all vectors with a Euclidean norm of $1$:
\begin{align}
B_p &= \left\{ v \in \mathbb{R}^p, \quad \text{s.t.} \, ||v||_2 = 1 \right\}.
\end{align}
Argue that:
\begin{align}
\min_{v \in B_p} \left\{ || X v ||_2^2 \right\} = \sigma_p^2
\end{align}
And
\begin{align}
\max_{v \in B_p} \left\{ || X v ||_2^2 \right\} = \sigma_1^2.
\end{align}
\item Finally, complete the questions in the file \texttt{class05.Rmd}.

% \item Let $A$ be a square $p$-by-$p$ matrix and consider the following system
% of linear equations:
% \begin{align}
% y = A b.
% \end{align}
% In the typical set-up, we would be given $y$ and $A$ and are asked to solve for
% the $p$ unknown quantities in $b$. Represent the SVD of $A$ as $U D V^t$ and
% assume that the true $b$ is equal to a column of $V$ ($V_k$)... though we are
% not allowed to use this information in solving the

% \item Not all matrices have an inverse; in fact, according to our definitions,
% it is not even possible for a non-square matrix to have an inverse. However,
% there are generalizations of matrix inverses that can be applied more broadly.
% Specifically, the Mooreâ€“Penrose inverse of the matrix $X$ is written as $X^{+}$
% and given by:
% \begin{align}
% X^{+} &= V D^{-1} U^t
% \end{align}
% Where $UDV^t$ is the SVD of the matrix $X$. Note that we have shown that
% $\beta = X^{+} y$. The Moore-Penrose inverse has the following four properties:
% \begin{align}
% X X^{+} X = X \\
% X^{+} X X^{+} = X \\
% (X X^{+})^{t} = X X^{+} \\
% (X^{+} X)^{t} = X^{+} X \\
% \end{align}
% Prove that these four properties hold given our decomposition of the SVD.
\end{enumerate}

\end{document}

