%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt,hidelinks]{article}

% 1. Load LaTeX packages
\usepackage{fontspec}
\usepackage{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xunicode}
\usepackage{listings}
\usepackage{color}
\usepackage{amssymb}

% 2. Define page dimensions and spacing
\geometry{top=1in, bottom=1in, left=1in, right=2in, marginparsep=4pt,
          marginparwidth=1in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

% 3. Set header, footer, and bibliography
\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancyplain}
\fancyhf{}
\lfoot{}
\rfoot{page \thepage\ of \pageref{LastPage}}
\bibliographystyle{acm}

% 4. Set fonts for the document
\defaultfontfeatures{Mapping=tex-text}
\setromanfont{YaleNew}

% 5. Define custom code for book environments and commands
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textbf{#1}}

% 6. Define custom code for book environments and commands
\definecolor{verbgray}{gray}{0.9}
\definecolor{verbgray2}{gray}{0.975}

\lstnewenvironment{rcode}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

\lstnewenvironment{rres}{%
  \lstset{backgroundcolor=\color{verbgray2},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

% 7. Define numbering scheme for equations (only needed for handout).
\numberwithin{equation}{section}
\setcounter{section}{7}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

{\LARGE Handout 07: Lasso Regression}

\vspace*{18pt}

Last time, we looked at adding a penalty term to our loss function to prefer
smaller regression vectors over larger ones. Adding an $\ell_2$-penalty leads
to the ridge regression, which has some nice properties. For example, we can
write down an analytic expression for the form of the regression vector and
could prove (though we did not) that it does an ideal job of minimizing the
variance of estimated regression vector.

Today we will look at two other penalties that could be added to the sum of
squared residuals. The first is called the $\ell_0$-norm, though it is not
in fact a vector norm. It counts the number of non-zero terms in a vector:
\begin{align}
|| b ||_0 &= \# \{ j \quad \text{s.t.} b_j \neq 0 \}.
\end{align}
Adding this to the least squares estimator leads to best subset regression:
\begin{align}
\beta^{BSR}_{\lambda} &= \argmin_b \left\{ || y - Xb||_2^2 + \lambda ||b||_0 \right\}
\end{align}
As another alternative, we can use the $\ell_1$-norm, given by the sum of
absolute values of the coordinates:
\begin{align}
|| b ||_1 &= \sum_j | b_j |.
\end{align}
This is a proper vector norm. Adding it to the square errors leads to the
lasso regression vector:
\begin{align}
\beta^{LASSO}_{\lambda} &= \argmin_b \left\{ || y - Xb||_2^2 + \lambda ||b||_1 \right\}
\end{align}
Best subset regression is useful when you have only a small number of variables.
For large datasets it is computationally intractable because the optimization
problem is not convex. The only way to find a solution is to check every single
combination of variables; the number of possibilities explodes beyond just a few
variables. The lasso regression does not have an analytic solution but can be
approximated using iterative methods; it is a convex optimization task. What
makes it so attractive is that it will do a form of subset selection that, in
practice, is nearly as good as the best subset selection.

Deriving the iterative solutions for the lasso regression problem is fairly
extensive and not applicable to many other applications. We will not get into
the details in this course. Today you are going to work with the simple case
where the columns of $X$ are uncorrelated:
\begin{align}
X^t X &= n \cdot 1_p. \label{uncor}
\end{align}
In this particular example it is possible to find analytic solutions to both
best subset selection and the lasso regression. I think it yields a lot of
motivation for understanding the behavior of the lasso in the more general
case.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\textbf{LAB QUESTIONS}

\vspace*{0pt}

\begin{enumerate}
\item This was a question from last lab, but in case you did not get there,
make sure that you can derive a formula for ridge regression under the uncorrelated
assumption in Equation~\ref{uncor}.
\item Write the best subset selection loss function as a sum over $j$ from $j=1$
to $j=p$. That is, you can write the loss function as a sum of independent terms,
where each elements depends only on the $j$'th column of $X$ and the $j$'th component
of $\beta$.
\item Repeat the previous question for lasso regression, showing that it also
decouples over the individual variables.
\item Understand why your answers to the last two questions allow us to minimize
the loss function individually for each component $\beta_j$.
\item Assume that $\beta_j \neq 0$. What would be its optimal value under best
subset regression? When is the loss at this point better than setting $\beta_j = 0$?
Put these conditions together to get a general formula for $\beta_j$ under best
subset selection.
\item Use a similar approach to find a solution for $\beta_j$ for lasso regression.
This requires a little bit more work is very doable with simple one-dimensional
calculus!
\item Consider a dataset for $n=100$ where Equation~\ref{uncor} holds and we have:
\begin{align}
X^t y &= n \cdot \begin{bmatrix} 10 \\ 3 \\ -5 \\ 1 \end{bmatrix}
\end{align}
Draw a sketch with $\lambda$ on the x-axis and $\beta_k$ for the best subset selection
on the y-axis. That is, you'll have four different curves showing the values of $\beta_1$,
$\beta_2$, $\beta_3$, and $\beta_4$.
\item Repeat the previous question for lasso regression.
\item Finally, repeat the sketch for ridge regression.
\end{enumerate}

\end{document}

