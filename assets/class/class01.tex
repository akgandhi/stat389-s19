%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt,hidelinks]{article}

% 1. Load LaTeX packages
\usepackage{fontspec}
\usepackage{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xunicode}
\usepackage{listings}
\usepackage{color}
\usepackage{amssymb}

% 2. Define page dimensions and spacing
\geometry{top=1in, bottom=1in, left=1in, right=2in, marginparsep=4pt,
          marginparwidth=1in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

% 3. Set header, footer, and bibliography
\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancyplain}
\fancyhf{}
\lfoot{}
\rfoot{page \thepage\ of \pageref{LastPage}}
\bibliographystyle{acm}

% 4. Set fonts for the document
\defaultfontfeatures{Mapping=tex-text}
\setromanfont{YaleNew}

% 5. Define custom code for book environments and commands
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textbf{#1}}

% 6. Define custom code for book environments and commands
\definecolor{verbgray}{gray}{0.9}
\definecolor{verbgray2}{gray}{0.975}

\lstnewenvironment{rcode}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

\lstnewenvironment{rres}{%
  \lstset{backgroundcolor=\color{verbgray2},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

% 7. Define numbering scheme for equations (only needed for handout).
\numberwithin{equation}{section}
\setcounter{section}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

{\LARGE Handout 01: Statistical Learning}

\vspace*{18pt}

\textbf{Introduction}

This book was written to supplement the existing literature in statistical
learning and predictive modeling. It provides a novel treatment of
the computational details underlying the application of predictive models to
modern datasets. It grew out of lecture notes from several courses we have
taught at the undergraduate and graduate level on linear models, convex
optimization, statistical computing, and supervised learning.

The major distinguishing feature of our text is the inclusion of code snippets
that give working implementations of common algorithms for
estimating predictive models. These implementations are written in the R
programming language using basic vector and matrix algebra routines. The goal
is to demystify links between the formal specification of an estimator and
its application to a specific set of data. Seeing the
exact algorithm used makes it possible to play around
with methods in an understandable way and experiment with how
algorithms perform on simulated and real-world datasets. This \textit{try and
see} approach fits a common paradigm for learning programming concepts.
The reference implementations also illustrate the run-time,
degree of manual tuning, and memory requirements of each method.
These factors are paramount in selecting the best methods in most
data analysis applications.

In order to focus on computational aspects of statistical learning,
we highlight models that can be understood as extensions
of multivariate linear regression. Within this framework, we show
how penalized regression, additive models, spectral clustering,
and neural networks fit into a cohesive set of methods for
the construction of predictive models built on core concepts from
linear algebra. The general structure
of our text follows that of the two popular texts \textit{An Introduction
to Statistical Learning} (ISL) \cite{james2013introduction} and
\textit{The Elements of Statistical Learning} (ESL)
\cite{friedman2001elements}. This makes our book a reference for
traditional courses that use either of these as a main text. In contrast
to both ISL and ESL, our text focuses on giving an in-depth analysis
to a significantly smaller set of methods, making it more conducive
to self-study as well as appropriate for second courses in linear models
or statistical learning.

\textbf{Computational approach}

\index{reference implementation}
In this text, we describe popular statistical models in terms of
their mathematical characteristics and the algorithms used to
implement them. We provide a reference implementation, in the R programming
environment, for each of the models considered. Code in this book is meant to
be read inline, just as you would read the text. Unlike pseudocode, our
reference implementations can be run, tested, and directly modified. However,
the code is not optimized and does not include the checks and error handling
one might expect from production-ready code. It is meant to be simple and
readable.

Our computational approach leads to a different presentation compared to
traditional approaches in statistical pedagogy where theory is separated from
practice (applying functions to datasets). While
the two-pronged approach clarifies the capabilities and
underpinnings of the field, it is in the space between these prongs that
researchers in the field usually find inspiration.
New models and approaches are often developed iteratively and empirically
while working with real datasets. Each step is justified mathematically only
after it is found to be effective in practice. Our approach bridges
statistical theory and model building by showing how they are related
through their implementation.

Understanding the computational details behind statistical modeling algorithms
is an increasingly important skill for anyone who wants to apply modern
statistical learning methods. Many popular techniques do not
allow for simple algorithms that can be easily applied to any problem. For
example training neural networks with stochastic gradient
descent is closer to an art form than a push button algorithm that can
be obfuscated from the user. Nearly every chapter in the text shows how
understanding the algorithm used to estimate a model often provides essential
insight into the model's use cases and motivation. Additionally, increasingly
large data sources have made it difficult or impossible, from a purely
computational standpoint, to apply every model to any dataset. Knowledge of
the computational details allows one to know exactly what methods are
appropriate for a particular scale of data.

\vspace{12pt}
\textbf{Statistical learning}

Statistical learning is the process of teaching computers to ``learn'' by
automatically extracting knowledge from available data. It is closely
associated with, if not outright synonymous to, the fields of pattern
recognition and machine learning. Learning occupies a prominent place within
artificial intelligence, which broadly encompasses all forms of computer
intelligence, both hand coded and automatically adapted through observed data.

\index{supervised learning}
We focus in this text on the subfield of \textit{supervised learning}.
The goal is to find patterns in available
inputs in order to make accurate predictions on new, unseen data. For this
reason models used in supervised learning are often called \textit{predictive
models}. Take the task of building an automatic spam filter.
As a starting point, we could label a small dataset of messages by hand. Then,
a statistical learning model is built that discovers what features in the
messages are indicative of the message being labeled as spam. The model can
be used to automatically classify new messages without manual
intervention by the user. Many applications can be written as supervised
learning tasks:
\begin{itemize}
\item Estimate how much a house will sell for based on properties such as the
number of bedrooms, location, and its size.
\item Determine whether a mass detected in an MRI scan is malignant or benign.
\item Predict whether a flight will be delayed given the carrier, scheduled
departure time, the departure airport, and the arrival airport.
\item Estimate the number of page views a website will need to handle next
month.
\item Given a picture of a person, predict their age and mood.
\item Determine the correct department to forward an online form request sent
to a company's help desk.
\item Predict the increased sales resulting from a new advertising campaign.
\end{itemize}
Domain-specific expertise is essential to the successful construction and
deployment of statistical learning algorithms. However, most methods used for
learning a predictive model from preprocessed data consist of applying
general purpose training algorithms. The algorithms we cover in this text can
be applied to a wide range of tasks, including all of the above examples,
given the availability of sufficiently representative data for training.

\vspace{12pt}
\textbf{Example} \label{sec_slearn_intro}

It is useful to look at a concrete example of a statistical learning task.
Consider recording the number of capital letters used in the content of $18$
text messages and labeling whether the message is spam or ``ham'' (non-spam).
For example, assume we observed the following dataset listing the number of
capital letters followed by whether this is a spam message or not:
\begin{rcode} %ignore
(0, ham)  (0, ham)   (0, ham)  (1, ham)   (1, ham)  (1, spam)
(2, ham)  (2, spam)  (2, ham)  (2, spam)  (2, ham)  (4, ham)
(5, spam) (5, spam)  (6, spam) (6, ham)   (8, spam) (8, spam)
\end{rcode} %ignore
As one might expect, messages with a large number of capital letters are more
likely to be spam. Now, we will use this data to predict whether a new text
message is spam based on the number of capital letters that are used. A
straightforward model for this prediction
task would be to select a cutoff value $N$, and classify new messages with
$N$ or more capital letters as spam. We can apply supervised statistical
learning to select a value of $N$ by making use of the observed data.

What is a good value for the parameter $N$ based on the observed text data?
If we set $N$ equal to $2$, the first row of data will be categorized as ham
and the second two will be labeled as spam. This leads to one mistake in the
first row and five in the second two rows, for a total of six mistakes.
Setting it to $5$, where only the last row is labeled as spam, yields a better
error rate with only four mistakes. Looking exhaustively through all possible
splits illustrates that the choice of $5$ leads to the fewest errors, and
therefore would be a good choice for $N$. To test how well this model works,
finally, we could acquire an additional dataset of text messages and compute
the new error rate for our chosen split value. We will discuss all of these
details, with fully worked out examples, throughout the text.

\index{best-split}
Continuing with our example we
illustrate the concept of a reference implementation by writing a best split
algorithm for classification. Our function takes two vectors; one gives a
numeric set of numbers on which to classify and the second defines the two
classes we want to distinguish. It works by exhaustively testing each possible
value in the first vector as a possible split point, and returns the best
possible split value. If there is a tie, all possible best splits are returned
as values.

\begin{rcode}
# Compute most predictive split of the training data.
#
# Args:
#     x: Numeric vector with which to classify the data.
#     y: Vector of responses coded as 0s and 1s.
#
# Returns:
#     The best split value(s).
casl_utils_best_split <-
function(x, y)
{
  unique_values <- unique(x)
  class_rate <- rep(0, length(unique_values))
  for (i in seq_along(unique_values))
  {
    class_rate[i] <- sum(y == (x >= unique_values[i]))
  }
  unique_values[class_rate == max(class_rate)]
}
\end{rcode}

The reference implementation uses common functions available in the base
version of R. It should be readable, though perhaps not reproducible from
scratch, by anyone familiar with scientific programming. Representing the data
displayed in Section~\ref{sec_slearn_intro} as objects in R, we can test the
implementation to make sure that it returns the same best fit value we arrived
at by manually checking each split value.

\begin{rcode}
x <- c(0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 4, 5, 5, 6, 6, 8, 8)
y <- c(0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1)

casl_utils_best_split(x, y)
\end{rcode}
\begin{rres}
[1] 5
\end{rres}

We see that it does return the correct value of $5$. Exercises at the
end of this chapter provide an opportunity to further test and tweak the
\code{casl\_utils\_best\_split} function.

\vspace{12pt}
\textbf{Formalisms and terminology}

We conclude this introduction by mathematically formalizing the central
elements of supervised learning. The resulting language and terminology will
be useful as a reference throughout the text.

Assume that there exists some unknown function $f$ that maps
elements from a set $\Omega$ into a set $\mathcal{Y}$,
\begin{align}
f: \Omega \rightarrow \mathcal{Y}, \label{abstract_slearn}
\end{align}
and consider observing tuples, known as \textit{training data},
\begin{align}
\left\{ \omega_i, y_i = f(\omega_i) \right\}_{i},
\quad \omega_i \in \Omega, \, y_i \in \mathcal{Y},
\quad i = 1, \ldots n.
\end{align}
We will avoid writing out a formal definition of the (possibly) random nature
of $f$ and the (possibly) random process that generates each $\omega_i$.
Doing so here would overly divert from the main discussion; we will define the
random nature of the data generation process whenever necessary.

A supervised learning algorithm constructs an estimate $\widehat{f}$ of the
function $f$ using the training data. The goal is to minimize errors in
estimating $f$ on new data for some loss function $\mathcal{L}$.
When predicting a continuous response variable, such as an expected price,
common loss functions include the squared or absolute difference between the
observed and predicted values,
\begin{align}
\mathcal{L} (\widehat{f}(\omega_{new}), f(\omega_{new})) &=
\left| \widehat{f}(\omega_{new}) - f(\omega_{new}) \right|
\end{align}
When the set of responses $\mathcal{Y}$ is
finite, as it would be when building a spam prediction algorithm, a common choice
of $\mathcal{L}$ is to measure the proportion of incorrectly labeled new
observations,
\begin{align}
\mathcal{L} (\widehat{f}(\omega_{new}), f(\omega_{new})) &=
\begin{cases} 0, & \widehat{f}(\omega_{new}) = f(\omega_{new}) \\
1, & \text{otherwise}  \end{cases}
\end{align}
Depending on the application, more complex metrics can be used. When
evaluating a medical diagnostic algorithm, for instance, it may make more
sense to weight incorrectly missing a serious condition more heavily than
incorrectly diagnosing a serious condition.

In nearly all supervised learning tasks, the training data will either be
given as, or coercible to, a vector of real values. In other words, we can
write the set $\Omega$ in Equation~\ref{abstract_slearn} as $\mathbb{R}^p$ for
some number $p$. Similarly, the prediction task can usually be re-written such
that $\mathcal{Y}$ is equal to $\mathbb{R}$---in the case of a discrete set,
this can be done by associating each category with an integer-based index.
Then, by stacking the $n$ training inputs together, we have
\begin{align}
X &= \begin{pmatrix} \omega_1 \\ \vdots \\ \omega_n \end{pmatrix} \in
\mathbb{R}^{n\times p}, \quad y =
\begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix} \in \mathbb{R}^{n},
\quad y = f(X).
\end{align}
The matrix $X$ is known as the \textit{feature matrix}.
This simplification allows for us to draw on techniques from numerical
analysis, functional analysis, and statistics in the pursuit of predictive
models. A central theme of this text will be building and evaluating
supervised learning algorithms motivated by the study of properties of the
matrix $X$ and assumptions made regarding the function $f$.

If the success of a supervised learning algorithm is defined on data that is,
by definition, unavailable, how will it be possible to determine how well a
predictive model is able to estimate the function $f$? One approach is to take
the observed tuples of data available for building predictive models and
partition them into two subsets. Only one of these partitions is used as the
\textit{training set} to produce the estimate $\widehat{f}$. The remaining
partition, the \textit{testing set}, is used to evaluate how well the estimate
can make predictions on new data. More complex schemes operate similarly by
splitting the data multiple times (e.g., \textit{cross-validation}) or
include a third partition to allow for tuning hyperparameters in the model
estimation algorithm.

\index{Vapnik-Chervonenkis dimension}
When considering the construction of a predictive model, a key concern is the
desired capacity, or complexity, of a model building
algorithm. A formal definition
of a training algorithm's complexity is given by its Vapnik--Chervonenkis (VC)
dimension \cite{vapnik1971uniform}, though an informal understanding of
complexity will suffice here. A model that is overly complex will
\textit{overfit} to the training data; that is, $\widehat{f}$ will fit the
training data very closely but not be able to generalize well to the testing
data. Conversely, a model with low complexity may be too constrained to
provide a good approximation to $f$. In a probabilistic framework, this can
be seen as equivalent to a trade-off between \textit{bias} and
\textit{variance}. A model building algorithm with high complexity will have
a large variance (it may overfit, and is therefore highly sensitive to the
training data); if it has a complexity that is too low to approximate $f$, it
will provide systematically biased results for some inputs. The study and
control of model complexity, in both its numerical and probabilistic forms,
is a guiding theme throughout this text.

\renewcommand{\section}[2]{}%
\vspace{12pt}
\textbf{References}
\bibliography{bibfile}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\textbf{LAB QUESTIONS}

\vspace*{0pt}

\begin{enumerate}
\item Here is a thing!
\end{enumerate}

\end{document}

