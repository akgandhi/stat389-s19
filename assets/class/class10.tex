%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt,hidelinks]{article}

% 1. Load LaTeX packages
\usepackage{fontspec}
\usepackage{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xunicode}
\usepackage{listings}
\usepackage{color}
\usepackage{amssymb}

% 2. Define page dimensions and spacing
\geometry{top=1in, bottom=1in, left=1in, right=2in, marginparsep=4pt,
          marginparwidth=1in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

% 3. Set header, footer, and bibliography
\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancyplain}
\fancyhf{}
\lfoot{}
\rfoot{page \thepage\ of \pageref{LastPage}}
\bibliographystyle{acm}

% 4. Set fonts for the document
\defaultfontfeatures{Mapping=tex-text}
\setromanfont{YaleNew}

% 5. Define custom code for book environments and commands
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textbf{#1}}

% 6. Define custom code for book environments and commands
\definecolor{verbgray}{gray}{0.9}
\definecolor{verbgray2}{gray}{0.975}

\lstnewenvironment{rcode}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

\lstnewenvironment{rres}{%
  \lstset{backgroundcolor=\color{verbgray2},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  keepspaces=true,
  columns=fullflexible}}{}

% 7. Define numbering scheme for equations (only needed for handout).
\numberwithin{equation}{section}
\setcounter{section}{10}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

{\LARGE Handout 10: Convex Optimization}

\vspace*{18pt}

We say that a function $f: \mathbb{R}^p \rightarrow \mathbb{R} $ convex if
for any values $z_1, z_2 \in \mathbb{R}^p$ and quantity $t \in [0, 1]$ we have
\begin{align}
f(t z_1 + (1-t) z_2) \leq t f(z_1) + (1-t) f(z_2).
\end{align}
Replacing the inequality in Equation~\ref{convex_def} with a strict
inequality for $t \in (0, 1)$ yields a definition of a strict convexity. If
$p$ is equal to $1$ and $f$ is twice differentiable, convexity is equivalent to
requiring that the second derivative is always non-negative. The lab today
has you derive some of the most fundamental properties of these convex
functions.

Convex functions play an important role in statistical learning because many
of the loss functions that we use to describe models are convex. There is an
entire theory of convex optimization from which we are borrowing algorithms
to solve our optimization tasks. Most importantly, we will use the technique
of stochastic gradient descent in the methods we investigate after the second
exam.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\textbf{LAB QUESTIONS}

\vspace*{0pt}

\begin{enumerate}
\item The most important property of convexity is the establishment of a global
minimum. Assume that $f$ is a strictly convex function and $x_1$ is a \textit{local
minimum} of $f$. In other words, $f$ is minimized by $x_1$ over a neighborhood
of $x_1$. Assume that there exists a value $x_2$ that is the global minimum of
$f$. Show that if $x_1 \neq x_2$ then the definition of strict convexity makes
it impossible for $x_1$ to be a local minimum. Therefore, strictly convex
functions do not have local minima.
\item
\end{enumerate}

\end{document}

